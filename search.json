[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rajiv’s Blog",
    "section": "",
    "text": "Fast, Easy ML development on Ray\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nRaya. A thin wrapper over Ray for faster ML development\n\n\n\n\n\n\nMay 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTabbyQL. A visual query language for databases\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nDesign of TabbyQL. A visual alternative to SQL\n\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nA toy Datalog parser using Ohm and Glue.\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA toy Datalog parser using Ohm parser and Glue.\n\n\n\n\n\n\nOct 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\nAn Unnecessarily Long-Winded Introduction to the Essence of Datalog\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nIn this post, I’ll gradually build up to a naive implementation of the Datalog engine in Python\n\n\n\n\n\n\nSep 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataScript. A modern datastore for the browser\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nDataScript is an immutable database for application state management in the browser\n\n\n\n\n\n\nMay 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\nBrython. On replacing JavaScript with Python for front-end development\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nBrython is a Python implementation which runs on the browser\n\n\n\n\n\n\nApr 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to PureScript: Twitter Search API\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nLearn how to call a Twitter API using PureScript\n\n\n\n\n\n\nApr 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPureScript on AWS Lambda. Using Express and Serverless\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nRun PureScript 0.12 on AWS Lambda, using Express on the Serverless platform\n\n\n\n\n\n\nMar 31, 2020\n\n\n\n\n\n\n  \n\n\n\n\nReading code easily with immutable values(Pyrsistent).\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nUse Pyrsistent to create immutable values in Python for better code maintenance.\n\n\n\n\n\n\nMar 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Datalog(Bashlog) in Python.\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nIntroduction to Datalog(Bashlog) in Python.\n\n\n\n\n\n\nFeb 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThampi. A Serverless Machine Learning Prediction System.\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nThampi. A Serverless Machine Learning Prediction System.\n\n\n\n\n\n\nDec 1, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I don’t know how it came to me but it changed my life. I was in grad school and my coding style was “bang on the keyboard until it’s done”. Then I chanced upon Martin Fowler’s book on Refactoring. Before that, I did not know that there was ‘good’ code and ‘bad’ code.\nThat book started a journey where programming became a creative endeavour and improved me as a programmer. This journey has brought me to declarative programming languages. I am now investigating Datalog as a programming language. Bloom, Differential Datalog, MLog all inspire me. Is there a common ground? Check out at Mercylog :)"
  },
  {
    "objectID": "posts/datascript-python.html",
    "href": "posts/datascript-python.html",
    "title": "DataScript. A modern datastore for the browser",
    "section": "",
    "text": "This article was originally posted at https://blog.rajivabraham.com/posts/datascript-python"
  },
  {
    "objectID": "posts/datascript-python.html#there-is-no-spoon-neo.",
    "href": "posts/datascript-python.html#there-is-no-spoon-neo.",
    "title": "DataScript. A modern datastore for the browser",
    "section": "There is no spoon, Neo.",
    "text": "There is no spoon, Neo.\nCough, before we move on, DataScript is different from your conventional SQL datastores. In conventional SQL databases, when we wish to implement an entity e.g customer, we first create a table schema customer with the attributes: name and age for e.g.\nCREATE TABLE customer (\n    name varchar,\n    age int\n);\n\nAccording to Datomic, the backend database which DataScript is an in-memory implementation of, such an approach is rigid. Hey, I sense it but not well enough to defend it. So, let’s not have twitter wars on that one… yet.\nDataScript eschews storing data as separate entities in separate tables(like customer or person). Instead, one can see DataScript as a store for attributes and its values only. So, DataScript prescribes that we only specify the schema for the attributes itself. E.g. name is string, age is integer but do not group them at all at design time before hand. Instead, the application decides which attributes to group together for a particular instance of that entity.\nIn an extreme example to clarify the concepts, the application may decide to store just a customer’s name(Rajiv) and his eye color(black) and give the customer an id 1. For another customer, it may store her as id 3 and just store her age(40). Simplistically(and naively incorrect), you can see the database as a single god table with columns entity_id, attribute, value where each row can belong to a different (entity_id, attribute) composite key. E.g.\n\n\n\nentity_id\nattribute\nvalue\n\n\n\n\n1\nname\nRajiv\n\n\n3\nage\n40\n\n\n2\nname\nCanada\n\n\n1\neye_color\nblack\n\n\n\nWhat about the entity 2? He/She is named Canada? Must be a cutie unlike the dull Rajiv. But we don’t know, the application knows. It may not represent a person at all! In this example, the pair (entity id=2, attribute=name) could be one attribute and value for an instance of the entity country e.g. Canada(O Canada, our home and native land… True Patriot Love .. with your public healthcare you command!). Before you panic and ditch Datomic/DataScript, in practice, the attribute names contain the entity name as well. So it’ll be “:customer/name” instead of “name” for Rajiv and “:country/name” instead of “name” for Canada. Or you could still leverage a common name attribute, your choice. And there is better support for ids than the pitiful example I’m giving here. Also, Imposter Alert, I’m not an expert by any means. I’m learning by doing :). That’s my EULA. Please do check out the design principles behind Datomic/DataScript. I think Datomic is a masterpiece in database engineering.\nLet’s show you how to add data. For e.g., we can add two dictionaries(Igor and Ivan) to the database.\ndb = datascript.empty_db()\ndb1 = datascript.db_with(db, [{\":db/id\": 1,\n                                \"name\": \"Ivan\",\n                                \"age\": 17},\n                                {\":db/id\": 2,\n                                \"name\": \"Igor\",\n                                \"age\": 35}])\nYou’ll notice that datascript takes a database (db) and adds data to it and returns a new database db1. This is because databases in DataScript are immutable. Once created, you can’t change it. This makes debugging and reasoning about the code using a database easier. I’ve written a bit about the general concept of immutability here\nFor each dictionary, DataScript will add two rows(one for name and another for age) in the single god table. The way DataScript keeps track is with the entity id i.e. :db/id attribute name which is a reserved attribute name in DataScript.\nSo this may be represented in the god table like:\n\n\n\nentity_id\nattribute\nvalue\n\n\n\n\n1\nname\nIvan\n\n\n1\nage\n17\n\n\n2\nname\nIgor\n\n\n2\nage\n35\n\n\n\nFinally, the query. The query follows the Clojure style of a mixed list. If we want to know the age of the entity whose name is Igor, the query would look like [:find ?a :where [?e \"name\" \"Igor\"] [?e \"age\" ?a]]. The variables with a question mark are called logic variables. If you are familiar with inner joins in SQL, it’s very similar. We are saying that if there is some :db/id (i.e. ?e) whose name is Igor, for that value of ?e(i.e 2) find a relation for age (i.e. [?e \"age\" ?a]) and return his age(i.e ?a). We should get back 35. The query and the call to the database is:\nresult = datascript.q('[:find ?a :where [?e \"name\" \"Igor\"] [?e \"age\" ?a]]', db1)\nThe full example is below:\n<!doctype html>\n<html>\n\n<head>\n    <meta charset=\"utf-8\">\n    <script type=\"text/javascript\" src=\"brython.js\"></script>\n    <script type=\"text/javascript\" src=\"brython_stdlib.js\"></script>\n    <script src=\"https://github.com/tonsky/datascript/releases/download/0.18.10/datascript-0.18.10.min.js\"></script>\n</head>\n\n<body onload=\"brython(1)\">\n<script type=\"text/python\">\nfrom browser import window, alert\ndatascript = window.datascript\n\ndb = datascript.empty_db()\ndb1 = datascript.db_with(db, [{\":db/id\": 1,\n                                \"name\": \"Ivan\",\n                                \"age\": 17},\n                                {\":db/id\": 2,\n                                \"name\": \"Igor\",\n                                \"age\": 35}])\nresult = datascript.q('[:find ?a :where [?e \"name\" \"Igor\"] [?e \"age\" ?a]]', db1)\nalert(result)\n</script>\n</body>\n\n</html>\nOther notes for the above example: * We load the Brython libraries: brython.js and brython_stdlib.js to run Python in the browser. * We load the datascript library from GitHub using the script tag . Brython will automatically create a reference for datascript under the window object. So we can refer to the module using window.datascript * When you open igor-just-datascript.html in a browser, it should return [[35]]. The return value is always a list."
  },
  {
    "objectID": "posts/datascript-python.html#mercylog-datascript",
    "href": "posts/datascript-python.html#mercylog-datascript",
    "title": "DataScript. A modern datastore for the browser",
    "section": "Mercylog Datascript",
    "text": "Mercylog Datascript\nNow you could happily continue and query the database with strings like [:find ?a :where [?e \"name\" \"Igor\"] [?e \"age\" ?a]]. Personally, I find writing code as strings works for simple queries. But it’s not easily composable and reusable. So I wrote a Brython library called mercylog-datascript which allows us to use Python to construct the queries. So the same query in mercylog-datascript becomes\n# The original datascript style as str_query\nstr_query = '[:find ?a :where [?e \"name\" \"Igor\"] [?e \"age\" ?a]]'\n\n# mercylog-datascript style\nfrom mercylog_datascript import DataScriptV1\nm = DataScriptV1()\nA, E = m.variables('a', 'e')\nquery = m.query(find=[A], where=[[E, \"name\", \"Igor\"], [E, \"age\", A]])\nassert str_query == query.code()\nGranted, it’s more code than a simple string but I think when the queries become complex and begin to have reusable parts, this style may start paying off.\nAll you need to access the mercylog-datascript library is add the following script tag:\n<script src=\"https://github.com/RAbraham/mercylog-datascript/releases/download/v0.1.4/mercylog_datascript.brython.js\"></script>`\nThe full script is at igor-mercylog-datascript.html\n\nMercylog-DataScript Queries\nNow that you have seen an example, let me show you the current query feature set with a sample dataset of actors, directors and movies. The full dataset is here. Here is an elided subset.\n[\n {\":db/id\":  100,\n  \":person/name\": \"James Cameron\",\n  \":person/born\": \"1954-08-16\"},\n\n {\":db/id\":  131,\n  \":person/name\": \"Charles Napier\",\n  \":person/born\": \"1936-04-12\",\n  \":person/death\": \"2011-10-05\"},\n  ....\n\n  {\":db/id\":  200,\n  \":movie/title\": \"The Terminator\",\n  \":movie/year\":  1984,\n  \":movie/director\":  100,\n  \":movie/cast\":  [101,\n               102,\n               103],\n  \":movie/sequel\":  [207]},\n\n {\":db/id\":  201,\n  \":movie/title\": \"First Blood\",\n  \":movie/year\":  1982,\n  \":movie/director\":  104,\n  \":movie/cast\":  [105,\n               106,\n               107],\n  \":movie/sequel\":  [209]},\n....\n\n]\nHere is the code to access the raw dataset and a simple pattern access as above with the Igor example. The code is here. Below we want to know the birthdate of Linda Hamilton and it will return [['1956-09-26']]\n<!doctype html>\n<html>\n\n<head>\n    <meta charset=\"utf-8\">\n    <script type=\"text/javascript\" src=\"brython.js\"></script>\n    <script type=\"text/javascript\" src=\"brython_stdlib.js\"></script>\n    <script src=\"https://github.com/RAbraham/mercylog-datascript/releases/download/v0.1.4/mercylog_datascript.brython.js\"></script>\n\n    <script src=\"https://github.com/tonsky/datascript/releases/download/0.18.10/datascript-0.18.10.min.js\"></script>\n</head>\n\n<body onload=\"brython(1)\">\n<script type=\"text/python\">\nfrom browser import window, alert, console\ndatascript = window.datascript\n\nfrom mercylog_datascript import DataScriptV1\nm = DataScriptV1()\nimport urllib.request, json\ndata_file_url = 'https://raw.githubusercontent.com/RAbraham/mercylog-datascript-client/master/data.json'\nconsole.log('Loading File')\nwith urllib.request.urlopen(data_file_url) as url:\n    result = json.loads(url.read())\nconsole.log('End loading file')\n\ndb = datascript.empty_db()\ndb2 = datascript.db_with(db, result)\n\ne, name, born = m.variables('e', 'name', 'born')\n\nquery = m.query(find=[born], where=[[e, \":person/name\", \"Linda Hamilton\"], [e, \":person/born\", born]])\nq = query.code()\nresult = datascript.q(q, db2)\nalert(result)  # [['1956-09-26']]\n\n\n</script>\n</body>\n\n</html>\nHenceforth, I’ll only focus on the mercylog-datascript query builder and how it supports DataScript.\nLet’s start with something simple. How do we just get the id(i.e :db/id) of a person? As shown in entity.html, the following query returns [[102]] for Linda Hamilton.\ne = m.variables('e')\nquery = m.query(find=[e], where=[[e, \":person/name\", \"Linda Hamilton\"]])\nThat’s great, but sometimes you want to create a parameterized query i.e. a query that can be used with different values. Let’s generalize the query above and then we can use it for different values. We do this by adding a parameters key to our query function.\nIn parameterized_queries.html, you’ll see:\n# [:find ?e :in $ ?name :where [?e \":person/name\" ?name]]\ne, name = m.variables('e', 'name')\nquery = m.query(find=[e], parameters=[name], where=[[e, \":person/name\", name]])\nq = query.code()\nresult1 = datascript.q(q, db2, 'Linda Hamilton')\nresult2 = datascript.q(q, db2, 'Sylvester Stallone')\nAs you can see above, the same query can be used to query Linda Hamilton and Sylvester Stallone.\nSometimes, you don’t care about the entity, when doing a search. For e.g, you just want all the movie titles. In that case, mercylog-datascript provides the m._ variable.\nIn underscore.html, you’ll find:\n# [:find ?title :where [_ \":movie/title\" ?title]]\ntitle = m.variables('title')\nquery = m.query(find=[title], where=[[m._, \":movie/title\", title]])\nThis would return\n[['First Blood'], ['Terminator 2: Judgment Day'], ....  ['Terminator 3: Rise of the Machines']]\nIn attr.html, we show how to find out the attributes which are commonly associated with a particular attribute(i.e. “:person/name”). To reiterate, as DataScript does not have a fixed schema, it’s possible for a customer record c1 to only have a name but another record c2 to have a name and age.\n# DataScript Query: [:find ?attribute :where [?person \":person/name\"] [?person ?attribute]]\nperson, attribute = m.variables('person', 'attribute')\nquery = m.query(find=[attribute],\n                where=[[person, \":person/name\"],\n                       [person, attribute]])\n\nq = query.code()\nresult = datascript.q(q, db2) # [[':person/born'], [':person/name'], [':person/death']]\nAbove, we find out that an entity which has an attribute :person/name can also have one or more of [[':person/born'], [':person/name'], [':person/death']]. The slow reader who didn’t try to read this in the elevator may have noticed that previously our list in the where clause were of size three but when we do such kind of meta searches, we can just pass lists of size 2(e.g. [person, attribute])\nNow, you sigh and say, this is all good but no language is a language unless it allows you to create functions. In transformation.html, we see an example of passing a user defined function(get_age) to be executed against the database. In the code snippet below, notice [get_age(born), age]. You could see this as age = get_age(born) for different values of born and you could reuse that in your query. In our case, we just ask for it directly in find.\nSince it’s user defined, we add get_age to the parameters argument for query as well and pass get_age.function to the DataScript query engine as well.\n# DataScript Query:[:find ?age :in $ ?get_age ?name :where [?p \":person/name\" ?name] [?p \":person/born\" ?born] [(?get_age ?born) ?age]]\nfrom datetime import datetime\ncurrent_year = int(datetime.today().strftime('%Y'))\nget_age = m.function('get_age', lambda born: (current_year - int(born.split('-')[0])) )\n\nborn, p, age, name = m.variables('born', 'p', 'age', 'name')\nquery = m.query(find=[name, age],\n                parameters=[get_age, name],\n                where=[[p, \":person/name\", name],\n                       [p, \":person/born\", born],\n                       [get_age(born), age]])\n\nq = query.code()\nresult = datascript.q(q, db2, get_age.function, \"Richard Crenna\")\nalert(result) #[['Richard Crenna', 94]]\n\n\nOk, you say this is simple stuff, yawn, what about SQL like aggregate functions? DataScript has you covered. In aggregates.html, we show the use of inbuilt functions of DataScript i.e. agg.max(age). As a bonus, we now show how to combine that with a user defined function(get_age).\n# DataScript Query:[:find (max ?age) :in $ ?get_age :where [?p \":person/name\" ?name] [?p \":person/born\" ?born] [(?get_age ?born) ?age]]\nfrom datetime import datetime\ncurrent_year = int(datetime.today().strftime('%Y'))\nget_age = m.function('get_age', lambda born: (current_year - int(born.split('-')[0])) )\nagg = m.agg\nborn, p, age, name = m.variables('born', 'p', 'age', 'name')\nquery = m.query(find=[agg.max(age)],\n                parameters=[get_age],\n                where=[[p, \":person/name\", name],\n                       [p, \":person/born\", born],\n                       [get_age(born), age]])\n\nq = query.code()\nresult = datascript.q(q, db2, get_age.function)\nalert(result) # [[94]]\n\nWhat about SQL where like clauses? We have filters in DataScript too. It’s basically a user defined function in it’s own row. For e.g, in predicate2.html(because getting predicate1.html to work made me cry), I’m going to make a simple filter function called born_before_1950 to filter out the wiser actors.\n# DataScript Query: [:find ?person :in $ ?born_before_1950 :where [?person \":person/born\" ?birth_date] [(?born_before_1950 ?birth_date)]]\nperson, birth_date = m.variables('person', 'birth_date')\nborn_func = lambda x: int(x.split('-')[0]) < 1950\nborn_before_1950 = m.function('born_before_1950', born_func)\nquery = m.query(find=[person],\n                parameters=[born_before_1950],\n                where=[[person, \":person/born\", birth_date],\n                       [born_before_1950(birth_date)]])\nq = query.code()\nresult = datascript.q(q, db2, born_before_1950.function)\nalert(result) # [[148], [119], [146], [105], [136], [116], [135], [145], [111], [114], [118], [115], [147], [104], [139], [110], [133], [131], [140], [101], [123], [142], [137], [138], [106], [107], [113], [124], [130]]\n \nWe can leverage ClojureScript inbuilt functions too. As seen in inbuilt_function3.html(why 3?, you guessed it ;)), we can call inbuilt functions like <. You still leverage the m.function method but since you don’t pass your own user defined function, mercylog-datascript will assume that you are trying to call an inbuilt function. You can call that convenient or over smart, only time will tell.\n# DataScript Query: [:find ?title :where [?movie \":movie/title\" ?title] [?movie \":movie/year\" ?year] [(< ?year 1984)]]\ntitle, movie, year = m.variables('title', 'movie', 'year')\nlt = m.function(\"<\")\nquery = m.query(find=[title],\n                where=[[movie, \":movie/title\", title],\n                       [movie, \":movie/year\", year],\n                       [lt(year, 1984)]])\n\nq = query.code()\nalert(q)\nresult = datascript.q(q, db2)\nalert(result) # [['First Blood'], ['Alien'], ['Mad Max'], ['Mad Max 2']\n\nBut most of the fun happens when we link different sources of data together to make money. For e.g. you may call a service which gives you the box office numbers for popular movies and you want to link that with the data in your database and find out the corresponding directors.\nThis is similar to an inner join in SQL between two tables but in this case, one side is a ‘table’ in DataScript and the other side is your in-memory structure which you obtain after calling the API for the service. Suppose you store the result of the API call in an in-memory list(title_box_office_pairs below). You need to tell DataScript about it’s structure. So you specify it in the parameters argument as m.collection([title, box_office]) and then later pass title_box_office_pairs to datascript.q() later. Then you could use the logic variable title and use that to link to the data in the DataScript store. Code snippet below and full code here,\n# DataScript query: [:find ?director ?box_office :in $ [[?title ?box_office]] :where [?p \":person/name\" ?director] [?movie \":movie/director\" ?p] [?movie \":movie/title\" ?title]]\n\nmovie, p, title, box_office, director = m.variables('movie', 'p', 'title', 'box_office', 'director')\n# title_box_office_pairs below could have been obtained from some api call.\ntitle_box_office_pairs = [\n [\"Die Hard\", 140700000],\n [\"Alien\", 104931801],\n [\"Lethal Weapon\", 120207127],\n [\"Commando\", 57491000],\n]\nquery = m.query(find=[director,\n                      box_office],\n                parameters=[m.collection([title, box_office])],\n                where=[[p, \":person/name\", director],\n                       [movie, \":movie/director\", p],\n                       [movie, \":movie/title\", title]])\n\nq = query.code()\nresult = datascript.q(q, db2, title_box_office_pairs)\nalert(result) # [['Richard Donner', 120207127], ['Mark L. Lester', 57491000], ['John McTiernan', 140700000], ['Ridley Scott', 104931801]]\n\nFinally, we can define rules. Let’s say we want to make a rule: Two people are mates if their names match(hey, that’s a good reason to be mates, no?). In Datalog, one would write it like\nMate(E1, E2) <= Name(N, E1), Name(N, E2)\ni.e. any person E1 is a mate of any person E2 if they both have the name N. Again N here is a logic variable which could represent all the names in the database. In mercylog-datascript, you would write the above as:\ne1, e2, n = m.variables('e1', 'e2', 'n')\n\nmate = m.relation('mate')\nr = m.rule(mate(e1, e2), [[e1, \"name\", n],\n                          [e2, \"name\", n]])\nA complete code listing is below. We also show to add a m.function to choose those rows where e1 has a bigger id than e2. I know! It does not make sense but I’ve been labouring on this post for three weeks and it’s time to wrap up :P:\nNOTE: There is some boilerplate code for now to send the rule as a parameter to the query:rule_code = '[' + r.code() + ']'. I’ll look into improving it if a thousand of you star this project on Github(Yeah, ain’t going to happen, I know :))\nfrom mercylog_datascript import DataScriptV1\nm = DataScriptV1()\ndb = datascript.empty_db()\n\ndb2 = datascript.db_with(datascript.empty_db({\"age\": {\":db/index\": True}}),\n                 [{ \":db/id\": 1, \"name\": \"Ivan\", \"age\": 15 },\n                  { \":db/id\": 2, \"name\": \"Petr\", \"age\": 37 },\n                  { \":db/id\": 3, \"name\": \"Ivan\", \"age\": 37 }]);\n\ne1, e2, p, title, n = m.variables('e1', 'e2', 'p', 'title', 'n')\nmate = m.relation('mate')\ngt = m.function('<')\nquery = m.query(find=[e1, e2],\n                where=[mate(e1, e2),\n                       [gt(e1, e2)]])\nq = query.code()\nalert(q)\n\nr = m.rule(mate(e1, e2), [[e1, \"name\", n],\n                          [e2, \"name\", n]])\nrule_code = '[' + r.code() + ']'\nalert(rule_code)\nresult = datascript.q(q, db2, rule_code)\n\nalert(result) # [[1,3]]\n\nThat’s about it. If you came so far, I love you. I really do. Call me.\n\n\nIf you loved the above.\n\nDataScript has two more APIs in addition to the Datalog API:\n\nEntity API: I think this is similar in concept to graph database like query languages. I leave it to the reader to investigate\nPull API: Similar to GraphQL\n\nIf you want to learn about more about the DataScript syntax, check out this site\nThe Datomic Data Model which DataScript is inspired about is mentioned here"
  },
  {
    "objectID": "posts/the-essence-of-datalog.html",
    "href": "posts/the-essence-of-datalog.html",
    "title": "An Unnecessarily Long-Winded Introduction to the Essence of Datalog",
    "section": "",
    "text": "+++\nThis is a better formatted post. Don’t read below.\n+++"
  },
  {
    "objectID": "posts/the-essence-of-datalog.html#datalog-concepts",
    "href": "posts/the-essence-of-datalog.html#datalog-concepts",
    "title": "An Unnecessarily Long-Winded Introduction to the Essence of Datalog",
    "section": "Datalog Concepts",
    "text": "Datalog Concepts\nLet’s start with a simple Datalog Program.\nWe can have simple facts in our database. e.g. Bob is a man. Abe is a man. In Datalog, we write it as:\nman(\"Bob\")\nman(\"Abe\")\nman is like a table in a database. man(\"Bob\") is a relation in that table. We’ll also call it a base relation.\nNext, we create the business logic i.e. rules.\nSomeone is a person if he is a man person(X) :- man(X)\nperson(X) is a derived relation, as it is derived from some base relation man(X). This is similar to a view in the database.\nX is a logical variable. So man(X) could be used to refer to all man relations in the database.\n{code-cell} ipython3 from typing import * from dataclasses import dataclass from pprint import pprint\nWe can create a logical variable like this:\n{code-cell} ipython3 @dataclass(frozen=True) class Variable:     name: str\n{code-cell} ipython3 X = Variable('X')\nA relation could be:\nman(\"Bob\")\nparent(\"John\", \"Chester\") # John is a parent of Chester\nIt could also be components of a rule e.g. man(X) or person(X) in person(X) :- man(X)\nSo a relation has a name(e.g. parent) and a list of attributes(e.g. \"John\" and \"Chester\" or X).\n```{code-cell} ipython3 @dataclass(frozen=True, eq=True) class Relation: ““” man(“Bob) is Relation(”man”, (“Bob”,)) # (“Bob”,) is a single valued tuple parent(“John”, “Chester”) is Relation(“parent”, (“John”, “Chester”)) man(X) is Relation(“man”, (Variable(“X”),))\n\"\"\"\nname: str\nattributes: Tuple\n\nA rule could be:\n- `person(X) :- man(X)` i.e. `X` is a person if he is man.\n- `father(X, Y) :- man(X), parent(X, Y)` i.e. `X` is a father of `Y` if `X` is a man and `X` is the parent of `Y`\n\nA rule has:\n- a head relation which is on the left of the `:-` symbol e.g. `person(X)` and `father(X, Y)` above\n- a body of relations which is on the right of the `:-` symbol e.g. `man(X)` and `man(X), parent(X, Y)` above\n\nSince Datalog is declarative, the order of the relations in the body does not matter. Both the statements below have the same meaning:\n\n`father(X, Y) :- man(X), parent(X, Y)`\n\n`father(X, Y) :- parent(X, Y), man(X)` # reversing the order does not matter\n\nSo, the body can be represented as a set\n\n```{code-cell} ipython3\n@dataclass(frozen=True, eq=True)\nclass Rule:\n    head: Relation\n    body: Set[Relation]\nThe last element of Datalog is the query. The simplest query is no rules, just facts.\nGiven:\nman(\"Abe)\nman(\"Bob\")\nwoman(\"Abby\")\nA query could be: man(X) # Find me all men\nThe query should return: {man(\"Bob\"), man(\"George\")}\nThis would be similar to a SQL query select * from man\n+++"
  },
  {
    "objectID": "posts/the-essence-of-datalog.html#simple-relation-query",
    "href": "posts/the-essence-of-datalog.html#simple-relation-query",
    "title": "An Unnecessarily Long-Winded Introduction to the Essence of Datalog",
    "section": "Simple Relation Query",
    "text": "Simple Relation Query\n+++\nThe simplest query is to find a matching relation. Taking the above example, let’s code that up in Python.\n{code-cell} ipython3 X = Variable('X') abe = Relation(\"man\", (\"Abe\",)) bob = Relation(\"man\", (\"Bob\",)) abby = Relation(\"woman\", (\"Abby\",)) database = {abe, bob, abby} no_rules = []  query = Relation(\"man\", (X,))\nFor some function run_simplest, I expect: assert run_simplest(database, no_rules, query) == {abe, bob}\nThe simplest run would iterate through all the facts and filter those facts that match the query by relation name.\n```{code-cell} ipython3 def name_match(fact: Relation, query: Relation) -> bool: return fact.name == query.name\ndef filter_facts(database: Set[Relation], query: Relation, match: Callable) -> Set[Relation]: return {fact for fact in database if match(fact, query)}\ndef run_simplest(database: Set[Relation], rules: List[Rule], query: Relation) -> Set[Relation]: return filter_facts(database, query, name_match)\nassert run_simplest(database, no_rules, query) == {abe, bob}\n\nLet's add some facts of length two:\n\nparent(“Abe”, “Bob”) # Abe is a parent of Bob parent(“Abby”, “Bob”) parent(“Bob”, “Carl”) parent(“Bob”, “Connor”) parent(“Beatrice”, “Carl”)\n\nI may want to query who are the parents of Carl\n\n\n`parent(X, \"Carl\")` should return `{parent(\"Bob\", \"Carl\"), parent(\"Beatrice\", \"Carl\")}`\n\n`parent(X, \"Carl\")` is similar to `select * from parent where child = \"Carl\"` if there was a table `parent` with columns `parent` and `child`)\n\nThe beauty of Datalog is that you can ask the inverse without additional code e.g. Who are the children of Bob\n\n`parent(\"Bob\", X)` should return `{parent(\"Bob\", \"Carl\"), parent(\"Bob\", \"Connor\")}`\n\n\nLet's code that up. Also from now on, I'm going to make a helper functions to make it easy to express relations like the lambda `parent` below.\n\n```{code-cell} ipython3\nparent = lambda parent, child: Relation(\"parent\", (parent, child))\ndatabase = {\n    parent(\"Abe\", \"Bob\"), # Abe is a parent of Bob\n    parent(\"Abby\", \"Bob\"),\n    parent(\"Bob\", \"Carl\"),\n    parent(\"Bob\", \"Connor\"),\n    parent(\"Beatrice\", \"Carl\")\n}\nNow, to the implementation. For a query to match, an argument at position N in the query should match the argument at position N in the fact. For e.g assert query_variable_match(parent(\"A\", \"Bob\"), parent(X, \"Bob\") ) == True\nLogical variables are special. They get a free pass like X above.\n```{code-cell} ipython3 def query_variable_match(fact: Relation, query: Relation) -> bool: if fact.name != query.name: return False\n# TODO: zip is duplicated?\nfor query_attribute, fact_attribute in zip(query.attributes, fact.attributes):\n    if not isinstance(query_attribute, Variable) and query_attribute != fact_attribute:\n            return False\nreturn True  \nassert query_variable_match(parent(“A”, “Bob”), parent(X, “Bob”) ) == True assert query_variable_match(parent(“A”, “Bob”), parent(“A”, X)) == True assert query_variable_match(parent(“A”, “NoMatch”), parent(X, “Bob”) ) == False\ndef run_with_filter(database: Set[Relation], rules: List[Rule], query: Relation) -> Set[Relation]: return filter_facts(database, query, query_variable_match)\n\nSo does it work?\n\n```{code-cell} ipython3\nparents_carl =  run_with_filter(database, [], parent(X, \"Carl\")) \nassert parents_carl == {parent(\"Bob\", \"Carl\"), parent(\"Beatrice\", \"Carl\")}\n\nchildren_bob =  run_with_filter(database, [], parent(\"Bob\", X)) \nassert children_bob == {parent(\"Bob\", \"Carl\"), parent(\"Bob\", \"Connor\")}"
  },
  {
    "objectID": "posts/the-essence-of-datalog.html#simple-rule-query",
    "href": "posts/the-essence-of-datalog.html#simple-rule-query",
    "title": "An Unnecessarily Long-Winded Introduction to the Essence of Datalog",
    "section": "Simple Rule Query",
    "text": "Simple Rule Query\nLet’s add a rule to our program.\nhuman(X) :- man(X) # You are human if you are man.\nAn example database below:\nman(\"Bob\")\nman(\"George\")\nanimal(\"Tiger\")\nQuery:\nhuman(X) # Find me all humans\nThe query should return: {human(\"Bob\"), human(\"George\")}\n```{code-cell} ipython3 man = lambda x: Relation(“man”, (x,)) animal = lambda x: Relation(“animal”, (x,)) human = lambda x: Relation(“human”, (x,)) X = Variable(“X”)\nhead = human(X) body = [man(X)] human_rule = Rule(head, body) # No pun was intended database = { man(“Abe”), man(“Bob”), animal(“Tiger”) } rules = [human_rule] query = human(X)\n\nFor each rule, for each relation in it's body, if it matches with any of the facts in the database,\nthen get the attributes of that fact and create a derived relation with those attributes.\nE.g., since we have `man(\"Abe\")` and our rule `human(X) :- man(X)`, we add a derived relation to our database `human(\"Abe\")`\n\n```{code-cell} ipython3\ndef match_relation_and_fact(relation: Relation, fact: Relation) -> Optional[Dict]:\n    if relation.name == fact.name:\n        return dict(zip(relation.attributes, fact.attributes))\n\ndef match_relation_and_database(database: Set[Relation], relation: Relation) -> List[Dict]:\n    inferred_attributes = []\n    for fact in database:\n        attributes = match_relation_and_fact(relation, fact)\n        if attributes:\n            inferred_attributes.append(attributes)\n    return inferred_attributes\n\n\ndef evaluate_rule_simple(rule: Rule, database: Set[Relation]) -> Set[Relation]:\n    relation = list(rule.body)[0] # For now, our body has only one relation\n    all_matches = match_relation_and_database(database, relation)\n    # We use the Python feature below that if we call `values` on a dictionary, \n    # it will preserve the order that was given when the dictionary was created\n    # i.e. in the `zip` inside `match_relation_and_database`. Thank God.\n    return {Relation(rule.head.name, tuple(attributes.values())) for attributes in all_matches}\nThis evaluate_rule_simple can be passed to a function which will evaluate it on each rule for the database to generate the final knowledge base.\n{code-cell} ipython3 def generate_knowledgebase(evaluate: Callable, database: Set[Relation], rules: List[Rule]):     knowledge_base = database      for rule in rules:         evaluation = evaluate(rule, database)         knowledge_base = knowledge_base.union(evaluation)     return knowledge_base\nAnd finally, we have\n```{code-cell} ipython3 def run_rule_simple(database: Set[Relation], rules: List[Rule], query: Relation): knowledge_base = generate_knowledgebase(evaluate_rule_simple, database, rules) return filter_facts(knowledge_base, query, query_variable_match)"
  },
  {
    "objectID": "posts/the-essence-of-datalog.html#logical-or-query",
    "href": "posts/the-essence-of-datalog.html#logical-or-query",
    "title": "An Unnecessarily Long-Winded Introduction to the Essence of Datalog",
    "section": "Logical OR Query",
    "text": "Logical OR Query\n+++\nLogical OR is just specifying two separate rules with the same head. E.g.\nhuman(X) :- man(X)\nhuman(X) :- woman(X)\nIn Python, given:\n```{code-cell} ipython3 database = { animal(“Tiger”), man(“Abe”), man(“Bob”), woman(“Abby”), woman(“Beatrice”) }\nman_rule = Rule(human(X), {man(X)}) woman_rule = Rule(human(X), {woman(X)}) rules = [man_rule, woman_rule] query = human(X)\nassert run_logical_operators(database, rules, query) == { human(“Abe”), human(“Bob”), human(“Abby”), human(“Beatrice”) }\n\n## Recursive Relations Query\n\n+++\n\nNext, we introduce the reason why we are interested in Datalog. Datalog intuitively captures hierarchies or recursion. E.g. we want to find all who are ancestors of someone.\nGiven:\nparent(“A”, “B”) # A is the parent of B parent(“B”, “C”) parent(“C”, “D”) parent(“AA”, “BB”) parent(“BB”, “CC”)\n\nA parent X of Y is by definition an ancestor.\n\n`ancestor(X, Y) :- parent(X, Y)`\n\nIf you are a parent of Y and Y is an an ancestor, then you are an ancestor as well.\n\n`ancestor(X, Z) :- parent(X, Y), ancestor(Y, Z)`\n\nQuery: `ancestor(X, Y)` should return all the parents above as ancestors \nancestor(“A”, “B”) # A is the ancestor of B ancestor(“A”, “C”) # A -> B -> C ancestor(“A”, “D”) # A -> B -> C -> D ancestor(“B”, “C”) ancestor(“B”, “D”) # B -> C -> D ancestor(“C”, “D”) ancestor(“AA”, “BB”) ancestor(“AA”, “CC”) # AA -> BB -> CC ancestor(“BB”, “CC”)\nIn Python,\n\n```{code-cell} ipython3\nancestor = lambda ancestor, descendant: Relation('ancestor', (ancestor, descendant))\n\ndatabase = {\n    parent(\"A\", \"B\"), \n    parent(\"B\", \"C\"), \n    parent(\"C\", \"D\"), \n    parent(\"AA\", \"BB\"),\n    parent(\"BB\", \"CC\")\n}\n\n\nX = Variable(\"X\")\nY = Variable(\"Y\")\nZ = Variable(\"Z\")\n# ancestor(X, Y) :- parent(X, Y)\nancestor_rule_base = Rule(ancestor(X, Y), [parent(X, Y)])\n# ancestor(X, Z) :- parent(X, Y), ancestor(Y, Z)\nancestor_rule_recursive = Rule(ancestor(X, Z), {parent(X, Y), ancestor(Y, Z)})\n\nrules = [ancestor_rule_base, ancestor_rule_recursive]\nAlright, let’s dive into this. What is different from run_logical_operator? It’s the hierarchy or recursion. If you see it as hierarchy(I’m visualizing this as a tree), one has to keep on going until we reach the top of the tree.\n\nSo let’s imagine how we would process the above example. In the first pass, we would do the simplest inference from base fact to derived fact using the base rule of ancestor(X, Y) :- parent(X, Y).\nShowing one hierarchy as an example(starting from A).\n\nPass 1: Base Facts and Inferred facts i.e. KnowledgeBase1\nparent(\"A\", \"B\"),\nparent(\"B\", \"C\"),\nparent(\"C\", \"D\"),\nparent(\"AA\", \"BB\"),\nparent(\"BB\", \"CC\")]\n# ----------------- New inferred facts below --------------\nancestor(\"A\", \"B\"),\nancestor(\"B\", \"C\"),\nancestor(\"C\", \"D\"),\nancestor(\"AA\", \"BB\"),\nancestor(\"BB\", \"CC\")\nNow that’s done, we can focus on inference from a combination of inferred facts and base facts to new inferred facts using the recursive rule ancestor(X, Z) :- parent(X, Y), ancestor(Y, Z). For e.g. in KnowledgeBase1, we have parent(\"C\",\"D\") and ancestor(\"B\", \"C\") , so we can infer the fact ancestor(\"B\", \"D\") i.e grandparents. We keep on doing this till we get:\n\nPass 2: KnowledgeBase2\nparent(\"A\", \"B\"),\nparent(\"B\", \"C\"),\nparent(\"C\", \"D\"),\nparent(\"AA\", \"BB\"),\nparent(\"BB\", \"CC\")\nancestor(\"A\", \"B\"),\nancestor(\"B\", \"C\"),\nancestor(\"C\", \"D\"),\nancestor(\"AA\", \"BB\"),\nancestor(\"BB\", \"CC\")\n# ----------------- New inferred facts below --------------\nancestor(\"A\", \"C\")\nancestor(\"B\", \"D\")\nancestor(\"AA\", \"CC\")\nDo we stop? No, we have to keep on going till we find all the ancestors. Let’s apply the rules to KnowledgeBase2 and get\n\nPass 3: KnowledgeBase3\nparent(\"A\", \"B\"),\nparent(\"B\", \"C\"),\nparent(\"C\", \"D\"),\nparent(\"AA\", \"BB\"),\nparent(\"BB\", \"CC\")\nancestor(\"A\", \"B\"),\nancestor(\"B\", \"C\"),\nancestor(\"C\", \"D\"),\nancestor(\"AA\", \"BB\"),\nancestor(\"BB\", \"CC\")\nancestor(\"A\", \"C\")\nancestor(\"B\", \"D\")\nancestor(\"AA\", \"CC\")\n# ----------------- New inferred facts below --------------\nancestor(\"A\", \"D\")\ni.e A is the great grand parent of D\nDo we stop? Yes(if you look at the above example), but the computer does not know that. There could be new inferred facts, so let’s try again for KnowledgeBase4.\nPass 4: KnowledgeBase4\nparent(\"A\", \"B\"),\nparent(\"B\", \"C\"),\nparent(\"C\", \"D\"),\nparent(\"AA\", \"BB\"),\nparent(\"BB\", \"CC\"),\nancestor(\"A\", \"B\"),\nancestor(\"B\", \"C\"),\nancestor(\"C\", \"D\"),\nancestor(\"AA\", \"BB\"),\nancestor(\"BB\", \"CC\")\nancestor(\"A\", \"C\")\nancestor(\"B\", \"D\")\nancestor(\"AA\", \"CC\")\nancestor(\"A\", \"D\")\n# ----------------- New inferred facts below --------------\nNo New Facts\nAha! There are no more new inferred facts. If we do another pass on KnowledgeBase4, it would come out the same. So we can stop!\nSo the logic to stop would be: Take the output of each iteration. If it matches the input to that iteration, stop(as we did not learn any new inferred facts). If not a match, then run another iteration. Let’s call this method iterate_until_no_change.\n```{code-cell} ipython3 def iterate_until_no_change(transform: Callable, initial_value: Set) -> Set: a_input = initial_value\nwhile True:\n    a_output = transform(a_input)\n    if a_output == a_input:\n        return a_output\n    a_input = a_output\n\nNow, we already have `evaluate_logical_operators_in_rule`. That will be our `transform` function above. So putting this all together below.\n\n```{code-cell} ipython3\ndef run_recursive(database: Set[Relation], rules: List[Rule], query: Relation):\n    transformer = lambda a_knowledgebase: generate_knowledgebase(evaluate_logical_operators_in_rule, a_knowledgebase, rules)\n    knowledgebase = iterate_until_no_change(transformer, database)\n    return filter_facts(knowledgebase, query, query_variable_match)\nLet’s define the query\n```{code-cell} ipython3 query = ancestor(X, Y)\nrecursive_result = run_recursive(database, rules, query)\nexpected_result = { ancestor(“A”, “B”), ancestor(“B”, “C”), ancestor(“C”, “D”), ancestor(“AA”, “BB”), ancestor(“BB”, “CC”), ancestor(“A”, “C”), ancestor(“B”, “D”), ancestor(“AA”, “CC”), ancestor(“A”, “D”) }\nassert recursive_result == expected_result, f”{recursive_result} not equal to {expected_result}”\n\nLet's explore other queries we can ask.\nIs `AA` the ancestor of `C`?(No! Such an impolite question)\n\n```{code-cell} ipython3\nquery = ancestor(\"AA\", \"C\")\n\nassert run_recursive(database, rules, query) == set()\nWhat if I want to find all ancestors of C?\n{code-cell} ipython3 query = ancestor(X, \"C\") assert run_recursive(database, rules, query) == {ancestor(\"A\", \"C\"), ancestor(\"B\", \"C\")}\nWhat if I want to find who all are the descendants of AA. Again, use the same query but just reverse the order!\n{code-cell} ipython3 query = ancestor(\"AA\", X) assert run_recursive(database, rules, query) == {ancestor(\"AA\", \"BB\"), ancestor(\"AA\", \"CC\")}\nFinally, who are the intermediates between A and D i.e. B and C.\nZ is an intermediate of X and Y if X is it’s ancestor and Y is its descendant.\nintermediate(Z, X, Y) :- ancestor(X, Z), ancestor(Z, Y)\nIn Python,\n```{code-cell} ipython3 intermediate = lambda intermediate, start, end: Relation(“intermediate”, (intermediate, start, end)) intermediate_head = intermediate(Z, X, Y) intermediate_body = {ancestor(X, Z), ancestor(Z, Y)} intermediate_rule = Rule(intermediate_head, intermediate_body)\nrules = [ancestor_rule_base, ancestor_rule_recursive, intermediate_rule] query = intermediate(Z, “A”, “D”)\nassert run_recursive(database, rules, query) == {intermediate(“B”, “A”, “D”), intermediate(“C”, “A”, “D”)} ```\n+++\nThat’s it! If you found it interesting, all these ideas will be explored in Mercylog, a soon to be built Datalog inspired library in Python."
  },
  {
    "objectID": "posts/the-essence-of-datalog.html#extra-extra.-read-all-about-it",
    "href": "posts/the-essence-of-datalog.html#extra-extra.-read-all-about-it",
    "title": "An Unnecessarily Long-Winded Introduction to the Essence of Datalog",
    "section": "Extra Extra. Read All About It",
    "text": "Extra Extra. Read All About It\n\nThis post was inspired by this post.\nSQL does support recursion. I just find Datalog has a cleaner syntax.\nOne aspect of Datalog being declarative is that the order of rules does not matter either. So technically, instead of rules = [rule1, rule2], we could have used rules = frozenset([rule1, rule2]). The latter is a bit more clutter so I used simple lists."
  },
  {
    "objectID": "posts/raya.html",
    "href": "posts/raya.html",
    "title": "Fast, Easy ML development on Ray",
    "section": "",
    "text": "Data Scientists need to play with large models during development. Some challenges are:\n\nConflicting Dependencies: In applications with multiple models, one model may need Tensorflow 1.x (TF1) and another Tensorflow 2.x (TF2).\nLong Load Time: These models take time to load and it’s slow to make a small change and run a script every time even if the model hasn’t changed.\nScalable and Distributed. Even for development, sometimes a laptop is too small too host and run multiple models. It would be convenient to be able to quickly iterate over code but run them on a remote powerful machine(with GPUs) or remote cluster created for experimentation.\nEase of Use: The gold standard would be to be able to create a Python class and use that in a way to solve all problems mentioned above.\nCaching: Often models are ‘pure’. i.e. the same output for the same input. But naively, we still send prediction requests to the model and have to wait for the output which is time consuming. It would be great to use a cache and make that work in the solution that I propose"
  },
  {
    "objectID": "posts/raya.html#ray",
    "href": "posts/raya.html#ray",
    "title": "Fast, Easy ML development on Ray",
    "section": "Ray",
    "text": "Ray\nRay is a distributed framework for building ML applications. The power of Ray is that the same code can run efficiently on a laptop(multi-core) but also scale to huge clusters just by changing an environment variable.\nOne Ray concept is of a Ray Actor. For me, this is an simple alternative to creating a service endpoint(see Caveat: Ray Serve). For more details, refer to the Actor documentation but a short description is in the code is below. Adapting from their site\nimport ray\n@ray.remote # Specifies that this is a Ray Actor \nclass Counter(object):\n    def __init__(self):\n        self.value = 0\n\n    def increment(self, step):\n        self.value += step\n        return self.value\n\n    def get_counter(self):\n        return self.value\n\n# Create an actor object from this class.\n# When run on a distributed cluster, this object could be anywhere\n# on the cluster, perhaps on a more powerful server.\n# The distributed complexity is hidden from us. Note the `remote`\ncounter = Counter.remote()\n\n# Call the actor. Compared to our typical Python objects,\n# all calls to the distibuted actor object 'counter' are\n# asynchronous, i.e. non-blocking. \n# This is great for ML applications as often some operations can\n# take a lot of time and we can proceed with 'other work'.\n# `obj_ref` is a reference to the future output \n# of the computation `counter.increment` \n# which we can hold on to until we actually need the value\nobj_ref = counter.increment.remote(step=2)\n\ndo_other_work() # dummy method. not implemented \n\n\n# Ok, now we are ready and we want the output of `counter.increment`\nprint(ray.get(obj_ref)) # 2"
  },
  {
    "objectID": "posts/raya.html#ray-clusters",
    "href": "posts/raya.html#ray-clusters",
    "title": "Fast, Easy ML development on Ray",
    "section": "Ray Clusters:",
    "text": "Ray Clusters:\nNormally clusters are a remote set of machines. To run code on a remote cluster, the code has to be copied and sent to the Ray cluster in form of Ray jobs. Some of the info we need to provide are:\n\nthe code folder\nrequirements (i.e. installable by pip)\n(optional) environment variables\n\nThese information can be provided to a Ray Job as Ray’s ‘Runtime Environment’(docs) . We can create multiple isolated environments on the Ray cluster. e.g. one for TF1 and one for TF2. raya attempts to be a thin opinionated wrapper over it.\nNow, for running the Ray cluster on your laptop, the same concepts apply i.e. of packaging up the code folder and requirements etc. This in imho, is slightly slower than just setting up multiple virtual environments on your laptop with python -m venv venv_TF1 and python -m venv venv_TF2 for e.g. and trying to run different models but the multiple virtual environment locally idea does not work with Ray. But the Ray team has done some splendid engineering to only reload the changes on to the Ray cluster, so this may be good for practical purposes. See ‘Caching for speed’ here\nFor raya, we focus on providing a separate runtime environment per Actor only"
  },
  {
    "objectID": "posts/raya.html#caching",
    "href": "posts/raya.html#caching",
    "title": "Fast, Easy ML development on Ray",
    "section": "Caching",
    "text": "Caching\nEven if we have these models up as actors and they are long running, they mostly will have the same prediction for the same input. Since model predictions on CPUs can take seconds, why not cache the output? Only pylru seems to work in a Ray Actor. So..\n# caching_model/actor.py\n\nimport raya\nfrom pylru import lrudecorator # <--------------\nimport time\n\nclass CachingModelActor(raya.Actor):\n    def __init__(self):\n        print(\"================= In Caching Model init ====================================\")\n\n    @lrudecorator(100) # <-------------------\n    def act(self, name):\n        time.sleep(5)\n        print(\"================= In Caching Do ====================================\")\n        return f\"Hi {name}\"\nI put a sleep for 5 seconds to simulate a slow model. My trial file is:\n# trial_caching.py\nimport ray\nimport time\n\nray.init(namespace=\"serve\")\n\na = ray.get_actor(namespace=\"serve\", name=\"CachingModelActor\")\n\nst = time.time()\nref = a.act.remote(name=\"Rajiv1\")\nresult = ray.get(ref)\nprint(result)\nprint(f\"Time1:{time.time() - st}\")\n\nst = time.time()\nref = a.act.remote(name=\"Rajiv2\")\nresult = ray.get(ref)\nprint(result)\nprint(f\"Time2:{time.time() - st}\")\n\n# This should return quickly\nst = time.time()\nref = a.act.remote(name=\"Rajiv1\")\nresult = ray.get(ref)\nprint(result)\nprint(f\"Time1:{time.time() - st}\")\nRunning \"python $DEV_DIR/raya-trial/trial_caching.py gives\n\nHi Rajiv1\nTime1:5.054863452911377 # <---------- returned in 5 secs\nHi Rajiv2\nTime2:5.024395227432251\nHi Rajiv1\nTime1:0.0022411346435546875 # <--------------- returned in 0 secs\nThere are additional small features like copy_env_vars which will copy your local environment variables to the Ray Cluster"
  },
  {
    "objectID": "posts/thampi-introduction.html",
    "href": "posts/thampi-introduction.html",
    "title": "Thampi. A Serverless Machine Learning Prediction System.",
    "section": "",
    "text": "Thampi takes care of the DevOps work for machine learning prediction systems.\n\n\nData scientists(most of us, actually) want to focus on training their models. We just want the banana but find an entire gorilla holding on to it. DevOps. Our models are useless unless they are deployed to production. Building prediction servers is hard and often fledgling data science teams(mostly fledgling data science person) have to grapple with so many difficult questions.\n\nCloud or On Premise?\nHow to use Docker?\nHow do I create a web server?\nWhat about scaling and server failures?\nI use Mac but we have to create the server in Linux?\nSecurity updates, OS patches etc.\nKubernetes? Ahhh… just shoot me.\n\n\n\n\nThampi is an attempt to take care of the DevOps work required for prediction systems. With Thampi, you have:\n\nMinimal Devops. With a single command, create a web server that scales, is fault tolerant and zero maintenance(courtesy AWS Lambda and Zappa).\nFocus on your model. Work in Python to train your model. Then wrap your model in a class which inherits from thampi.Model and Thampi takes care of the rest.\nPip and restricted Conda support.\nWork on any platform*. Work on Mac(technically possible on other platforms but untested) and still deploy to AWS Lambda(which is Linux). As you know, if one’s working on Mac, one can’t just upload the machine learning libraries(e.g. scikit-learn) to a Linux server. That’s because most machine learning libraries, for performance reasons, are written in C and these C extensions are compiled to OS specific binaries.\nThampi alleviates this by using Docker underneath so that you can work on a Mac(or Windows?). When serving the model, it recreates your machine learning project in the docker image(which is Linux) and compiles all the C libraries(e.g. scikit-learn) for you to Linux binaries, which is then uploaded to AWS Lambda.\n\n\n\n\nLet’s use the Iris dataset provided within scikit-learn. Let’s create a new project myproject. We’ll use scikit-learn as an example but you could use any framework.\n\n\nmkdir myproject && cd myproject\nvirtualenv -p python3 venv\nsource ./venv/bin/activate\npip install thampi\npip install scikit-learn\npip install numpy\npip install scipy\npip freeze > requirements.txt\n\n\n\n\nNote: This is one way of creating a conda environment. Please use the conventional way if you are comfortable in that style.\nmkdir myproject && cd myproject\n# Create a  conda environment inside the directory myproject\nconda create --prefix=venv python=3.6.7\npip install thampi\npip install scikit-learn\npip install numpy\npip install scipy\n\nIMPORTANT: thampi only supports conda requirements files crafted by hand. So, let’s manually create a requirements file with the above dependencies as shown below and save it as requirements.txt. The versions will change but you get the idea.\nname: thampi-tutorial\ndependencies:\n  - thampi=0.1.0\n  - numpy=1.15.*\n  - scikit-learn=0.20.0\n  - scipy=1.1.0\n\n\n\n\n\nRun thampi init and you should see something similar to the terminal output below.\nFor the s3 bucket, you can choose to have one bucket for all your thampi applications. Each project(model) is at a different prefix so as long as the projects have unique names, they won’t overwrite each other. If you aren’t confident of that, you could just give a different bucket for each thampi project.\nChoose pip or conda according to your preference.\n\nthampi init\n\nWelcome to Thampi!\n-------------\nEnter Model Name. If your model name is 'mymodel', the predict endpoint will be myendpoint.com/mymodel/predict\nWhat do you want to call your model: mymodel\n-----------------\n \nAWS Lambda and API Gateway are only available in certain regions. Let's check to make sure you have a profile set up in one that will work.\nWe found the following profiles: analytics, and default. Which would you like us to use? (default 'default'): default\n------------\n \nYour Zappa deployments will need to be uploaded to a private S3 bucket.\nIf you don't have a bucket yet, we'll create one for you too.\nWhat do you want to call your bucket? (default 'thampi-2i1zp4ura'): thampi-store\n-----------------\nEnter package manager:['conda', 'pip'](default: pip):pip\nA file zappa_settings.json has been created. If you made a mistake, delete it and run `thampi init` again\n\n\nIt has created a file called zappa_settings.json. This file is used by the Zappa framework. You’ll note that some defaults have been filled up which are suitable for machine learning projects. A notable setting is keep_warm which prevents AWS Lambda from evicting the instance due to lack of use, by pinging the lambda(e.g. every 4 minutes). This is useful in the case when you have very large models. However, you could take it out if you feel that your model is small enough. For more details on how you can customize zappa_settings.json, check out zappa docs\nWithin zappa_settings.json, thampi adds a key thampi. All thampi specific settings will go here. Note: zappa has no idea of thampi. It’s just a convenient place to store the thampi relevant configuration.\n\n\n\n\n\nInside myproject, copy the following code into the file train.py\nimport numpy as np\nfrom sklearn import datasets\nfrom typing import Dict\nimport thampi\nfrom sklearn.neighbors import KNeighborsClassifier\n \nclass ThampiWrapper(thampi.Model):\n    def __init__(self, sklearn_model):\n        self.sklearn_model = sklearn_model\n        super().__init__()\n \n \n    def predict(self, args: Dict, context) -> Dict:\n        original_input = [args.get('input')]\n        result = self.sklearn_model.predict(np.array(original_input))\n        return dict(result=int(list(result)[0]))\n \ndef train_model():\n    iris = datasets.load_iris()\n    iris_X = iris.data\n    iris_y = iris.target\n    np.random.seed(0)\n    indices = np.random.permutation(len(iris_X))\n    iris_X_train = iris_X[indices[:-10]]\n    iris_y_train = iris_y[indices[:-10]]\n \n    knn = KNeighborsClassifier()\n    knn.fit(iris_X_train, iris_y_train)\n    return ThampiWrapper(knn)\n \n \nif __name__ == '__main__':\n    model = train_model()\n    thampi.save(model, 'iris-sklearn', './models')\n\n\nThe above code first trains the sklearn model as knn. To make the thampi web framework send the request data to the model, we wrap knn in ThampiWrapper, a class which implements the thampi.Model interface. The data sent to the serving endpoint will be passed by thampi to the predict method as args. Likewise, one can wrap models of other libraries as well. Ignore the context argument in the predict method for now. The context object sends in the Flask application object(and others in the future) which is probably not required for most of the use cases for now.\n\nAnd then at the terminal run\npython train.py\nThis will create the model and save it locally using thampi.save. In thampi, like mlflow, the model artifacts are stored in a directory(i.e. iris-sklearn). Storing it in the models directory is just arbitrary convention.\n\n\nNow it’s time to upload the model to AWS Lambda. All you have to provide is the requirements.txt file along with the above trained ./models/iris-sklearn directory.\nthampi serve staging --model_dir=./models/iris-sklearn --dependency_file=./requirements.txt\nThe serve command will use zappa internally to create or update a server endpoint. To see the endpoint, do\nthampi info staging\nYou’ll see something similar to:\n{'url': 'https://8i7a6qtlri.execute-api.us-east-1.amazonaws.com/staging/mymodel/predict'}\nLet’s hit the endpoint in the next section.\n\n\n\nYou can do a curl like below where you replace a_url with the url that you receive from thampi info staging\ncurl -d '{\"data\": {\"input\": [5.9, 3.2, 4.8, 1.8]}}' -H \"Content-Type: application/json\" -X POST a_url\ndata is a keyword here. Anything passed to data will be sent along to your model. The dictionary with the key input depends on your application. It could have been something else like features instead of input for e.g. If you remember from the ThampiWrapper code above, since we use input, our code reads the data as args.get('input')\nOutput:\n{\n  \"properties\": {\n    \"instance_id\": \"9dbc56dd-936d-4dff-953c-8c22267ebe84\",\n    \"served_time_utc\": \"2018-09-06T22:03:09.247038\",\n    \"thampi_data_version\": \"0.1\",\n    \"trained_time_utc\": \"2018-09-06T22:03:04.886644\"\n  },\n  \"result\": {\n    \"result\": 2\n  }\n}\nFor convenience, you can also do:\nthampi predict staging --data='{\"input\": [5.9, 3.2, 4.8, 1.8]}'\nwhere data is of json format.\nThe properties dictionary is meta-data associated with the model. Most of them are populated using the save command. If you want to add custom data (e.g name for your model and version, you can add it within tags)\n\n\n\n\nLike most AWS Lambda solutions, thampi has restrictions which hopefully most use cases fall within.\n\nConda support is only for dependency files crafted by hand\nMax 500MB disk space. Ensure that your project with it’s libraries is below this size.\nMax 900 MB model size. This number was derived from a few live tests. We circumvent the 500 MB disk space limit by loading the model directly from S3 to memory. thampi tries to reduce repeated calls to S3 by using zappa’s feature of pinging the lambda instance every 4 mins or so(configurable). In that way, the model will stay on the lambda instance(unless it’s a first time load or if AWS Lambda does decide to evict the instance for other reasons)\n\nBased on feedback, an option could be added to package the model with the code but you’ll then have to have a very small model size. It depends on what framework you use but generally you may have 100–250 MB space for your model as machine learning libraries take up a lot of space. Also look at the section on Design\n\n\n\n\n\nAWS SageMaker has the following advantages(not exhaustive):\n\nIf you have SLAs, then Sagemaker may be a good choice. As they are on demand instances, you won’t have the cold start delays.\nGPU Inference available\nCan serve models bigger than 1 GB(upon filling a form)\nYou can choose more powerful machines than what AWS Lambda can offer you.\n\nSagemaker has the following costs(correct me if I am wrong):\nIf you have to deploy a model which is not supported by Sagemaker(e.g. lightfm), then you have to:\n\ncreate your own docker image(manage your own OS updates e.g. security)\nimplement a web server which has to implement a few endpoints\nmanage auto scaling,\nprovide handlers to SIGTERM and SIGKILL events\nmanage some environment variables.\n\nFor more details, look here.\nthampi(via AWS Lambda) abstracts all this away from you.\n\n\n\nAlgorithmia is a commercial alternative to Thampi. I’m not sure though how easy it is to deploy models on libraries not supported by Algorithmia(e.g. lightfm). Note: I haven’t spend too much researching Algorithmia.\n\n\n\n\nHopefully, thampi satisfies a pain point for you. You can install it, Try the tutorial and let us know!"
  },
  {
    "objectID": "posts/brython.html",
    "href": "posts/brython.html",
    "title": "Brython. On replacing JavaScript with Python for front-end development",
    "section": "",
    "text": "This article was originally posted at https://blog.rajivabraham.com/posts/brython\n\nPurpose.\nThis blog article will: * give a brief introduction to using Brython, a Python implementation for front-end development on the browser\nThe entire project is here\n\n\nIntroduction\nJealous of JavaScript programmers, a cabal of Python programmers secretly met to discuss the future of Python in this apocalyptic world. JavaScript was everywhere and eating Python’s lunch. With Node.js, JavaScript had invaded Python territory and ended its dominance as everyone’s favorite language after Ruby(not very dominant then, is it? Mr. Author). It was time to make a thrust into the heart of JavaScript land: The Browser.\n\n\nDon’t forget your history(and future)\nThe cabal were not the only gentlemen concerned with this dilemma. The author of Transcrypt believed in poison and espionage. He decided to write a Python compiler which compiled to JavaScript code. Like good poison, there was no trace of Python. It looked very promising. But the author of this boring post thought he found a bug but later found out it wasn’t so. Unwisely, he relegated Transcrypt to a side plot and must keep it there for now.\nOthers wanted to learn from history. Just immigrate the entire family. At least, that’s what Pyodide thought of doing. Their strategy was to create an enclave on the side with a full Python Interpreter which can run Python code. Thus you could run any Python code including most of the data science stack which contains C language bindings (e.g. Numpy, Pandas).\nThis looks very promising too. But on initial lazy tests by this author, the initial page load was a bit slow(The real reason was that the author also could not find an easy way to make this work and was happy to not pursue it any further.)\nSo the cabal decided to do what every cabal is supposed to do i.e. create another Python to JavaScript compiler but this time, compile it to JavaScript when the page loads(unlike Transcrypt which compiles to JavaScript ahead of time). Thus, the fellowship of Brython was formed. One snake to rule them all.\n\n\nHello World\nLet’s code up the customary ‘Hello World’\nThe Brython paratroopers(compiler) is here.\n<script type=\"text/javascript\"\n       src=\"https://cdn.jsdelivr.net/npm/brython@3.8.9/brython.min.js\">\n</script>\nWe activate it on page load\n<body onload=\"brython()\">\n...\n</body>\nWithin the body tag above, we write the Brython Code:\n<script type=\"text/python\">\nfrom browser import document\n\ndocument <= \"Hello World\"\n</script>\nWe just add Hello World to the document element. Hmmm. That was easy.\nIn complete form, it’s shown below.\n<!doctype html>\n<html>\n<head>\n    <meta charset=\"utf-8\">\n    <script type=\"text/javascript\"\n        src=\"https://cdn.jsdelivr.net/npm/brython@3.8.8/brython.min.js\">\n    </script>\n</head>\n\n<body onload=\"brython()\">\n\n<script type=\"text/python\">\nfrom browser import document\n\ndocument <= \"Hello World\"\n</script>\n</body>\n</html>\nThis will simply print “Hello World” to the page.\n\n\nCalculator\nLet’s now make a calculator(code courtesy: Brython). The full code is here\n\n\n\n\n\nalt text\n\n\nYes, you were right. We do need a table. Let’s make one.\nfrom browser import document, html\ncalc = html.TABLE()\nLet’s add the first row only. Just the display box(we’ll name it result) and C.\ncalc <= html.TR(html.TH(html.DIV(\"0\", id=\"result\"), colspan=3) +\n                html.TD(\"C\"))\n \nYes, I’m not very sure of this <= syntax either. But hey, for such a lovely library, I’ll settle for it too :).\nLet’s now add the number pad\nlines = [\"789/\", \"456*\", \"123-\", \"0.=+\"]\ncalc <= (html.TR(html.TD(x) for x in line) for line in lines)\nFinally, we add calc to the document\ndocument <= calc\nNow that’s all good. How do we make it work? First, we need to capture a reference to the result element to manipulate it when the number pad is pressed.\nresult = document[\"result\"] # direct access to an element by its id\nNext, we need to update the result whenever any element in the number pad is clicked. Let’s make an event handler. We’ll trust the Brython developers that this code works. Notice the manipulation of result based on the button you clicked.\ndef action(event):\n    \"\"\"Handles the \"click\" event on a button of the calculator.\"\"\"\n    # The element the user clicked on is the attribute \"target\" of the\n    # event object\n    element = event.target\n    # The text printed on the button is the element's \"text\" attribute\n    value = element.text\n    if value not in \"=C\":\n        # update the result zone\n        if result.text in [\"0\", \"error\"]:\n            result.text = value\n        else:\n            result.text = result.text + value\n    elif value == \"C\":\n        # reset\n        result.text = \"0\"\n    elif value == \"=\":\n        # execute the formula in result zone\n        try:\n            result.text = eval(result.text)\n        except:\n            result.text = \"error\"\n\nFinally, we associate the event handler above to the click event of all buttons.\nfor button in document.select(\"td\"):\n    button.bind(\"click\", action)\n\nSee, how easy it is when someone else writes the code :P. But seriously, Brython is a wonderful work of engineering and perhaps the best display of programmer love for their beloved Python language. Please support the developers, at least with a star on their Github repo!\n\n\nFor the advanced reader\n\nOne can also integrate third party libraries like Vue.js as shown here.\nA great in depth explanation of the concepts can be found here"
  },
  {
    "objectID": "posts/purescript-twitter.html",
    "href": "posts/purescript-twitter.html",
    "title": "Introduction to PureScript: Twitter Search API",
    "section": "",
    "text": "This post is an ported, edited version of the original\nTLDR: I wrote this in a fiction format for fun. The actual code is in the repo. Also, I’m new to FP so this is newbie code. I can refactor it to be elegant but I want to keep this simple for beginners."
  },
  {
    "objectID": "posts/purescript-twitter.html#twitter-storm",
    "href": "posts/purescript-twitter.html#twitter-storm",
    "title": "Introduction to PureScript: Twitter Search API",
    "section": "Twitter Storm",
    "text": "Twitter Storm\nKim Kardashian felt uneasy as soon as she woke up. She had just used the r-word yesterday and suffered a huge backlash. She felt vulnerable about her twitter following and needed to be reassured. She had to do something different. Yes, she could just type her name in the Twitter App and see what people were saying about her. But she had secretaries for that. No, she had to do what no other celeb had done before. She would code!\nWhat language though? A language which is nice and clean and pure. So she googles around and discovers PureScript! She installs it in a breeze while wondering about this Mr. Java Script guy who was always complaining online on how difficult it was. Sigh. Ok, what next?"
  },
  {
    "objectID": "posts/purescript-twitter.html#reading-twitter-credentials",
    "href": "posts/purescript-twitter.html#reading-twitter-credentials",
    "title": "Introduction to PureScript: Twitter Search API",
    "section": "Reading Twitter credentials",
    "text": "Reading Twitter credentials\nFirst, she has to read her Twitter credentials from a file. Yes, she could hard code the passwords in the program but she’s a celeb. She knows Security.\nSo, she got her credentials from Twitter and created a file like below at config/twitter_credentials.json\n{\n  \"consumer_key\": \"KimMama\",\n  \"consumer_secret\": \"KimLikesToCode\",\n  \"access_token\": \"KimDoesNotKnowWhatThisIsFor\",\n  \"access_token_secret\": \"KimThinksTwitterHasGoneMad\"\n}\n\nShe built a JavaScript like object in PureScript(called records) using type:\ntype TwitterCredentials =\n  { consumer_key :: String\n  , consumer_secret :: String\n  , access_token :: String\n  , access_token_secret :: String\n  }\n\nHow do we read this file?\nimport Node.Encoding (Encoding(..))\nimport Node.FS.Aff (readTextFile)\n\nreadConfigStr :: String -> Aff String\nreadConfigStr path =  readTextFile UTF8 path\nimport Node.Encoding (Encoding(..)) meant import the type constructor Encoding and the .. meant import all it’s data constructors as well, one of which is UTF8. Since she is a celeb and she is never wrong, type constructors are like abstract base types and data constructors are like normal OOP constructors but fancier. You can have data constructors with different names and you can even treat them like Enumerations in switch/case like statements(Kim’s BFF liked to call them pattern matching).\nAff stands Asynchronous Effect(the synchronous effect is called Effect). These effects represent an action that the program would like to take, but not executed yet. Whaaa?\nIf Kim wanted to call Khloe for lunch, buy flowers for her mother and type her next tweet… She wouldn’t be the person doing it, would she? It would be her secretary! All, she would do is text her secretary commands to do this thing but it wouldn’t happen until her secretary actually executed the commands at a later time!\nIn the same way, Aff(and Effect) were like texts by Kim to her new secretary PureScript. It was a way of telling PureScript that she wanted them to be done but it was just a representation of a command, not the actual execution of the command. By representation, it just meant it was a value, just like the way number 3 or \"a_string\" or a JavaScript object were values.\nFor e.g., imagine the following pseudocode in an imperative language(e.g. Python):\n1: x = print(\"A String\")\n2: x\n3: x\n\nThe output would be\nA string\nThe execution and evaluation of the print statement both happen at line 1.\nBut in a functional language, the above pseudocode would be something like\n1: run(\n2: let x = print(\"A String\")\n3: x\n4: x\n5: )\nAnd the output would be\nA String\nA String\nlet is like the variable assignment in imperative code.\nOnly the evaluation happens at lines 2-4 but not the execution. The execution happens inside run. So before the program is given to run, x replaced at lines 3 and 4 to be print(\"A String\"). Note, the print has different interpretations. In the imperative setting, it executes a command, but in the functional setting, it executes nothing, just returns back a value representing an action for future execution by the run procedure.\nAnother viewpoint is that most applications always start with the main function. In PureScript, perhaps the simplest program one could write is.\nimport Effect.Console (log)\nmain:: Effect Unit\nmain = log \"Product Placement Here. ;)\"\nThe signature for log is log :: String -> Effect Unit. Unit stands for nothing, as in, we don’t expect anything back from the console.\nAnd like the pseudocode above, what happens within PureScript code, unseen by the programmer is something like\nrun(main)\nKim felt a chill through her spine. She regretted not taking programming seriously in school.\nOk, readConfigStr returned a Aff String but she needed to convert it to our TwitterCredentials record. She asked her secretary for technology to find a library for her and she found PureScript-Simple-JSON by a guy called Justin Woo.\nimport Simple.JSON as SimpleJSON\nimport Data.Either (Either(..))\n\nparseConfig :: String -> Either String TwitterCredentials\nparseConfig s =\n  case SimpleJSON.readJSON s of\n    Left error -> Left (show error)\n    Right (creds :: TwitterCredentials) -> Right creds\nparseConfig has an Either String TwitterCredentials in it’s signature. It’s like an union type. The result could either be a String(an error string) or the actual credentials. PureScript defines Either as\ndata Either a b = Left a | Right b\nSo if we want to return a string, we return Left \"my error string\", the actual credentials as Right creds. That way, the person calling parseConfig knows which is which.\nIn parseConfig, SimpleJSON.readJSON returned an Either but Kim didn’t want to deal with the complex Left type, so she just converted that to a string using show.\nNow it was just a matter of calling readConfigStr and passing the value to parseConfig. Something like this pseudocode\ncStr = readConfigStr path\nreturn parseConfig cStr\nBut she couldn’t make it compile! She started panicking and thought of what would happen if the word got out and Taylor Swift found out. The Shame\n“Try the do notation”, said a voice from behind.\nKim swivelled back and her mouth opened with surprise.\n“Kanye! I didn’t know you knew PureScript!”\n“Nah, PureScript is for hipsters. I’m old school. I like my Haskell.”\nHe continued, “The do notation allows you to extract the String from Aff String and gives you the illusion of the pseudocode above.”\nreadConfig :: String -> Aff (Either String TwitterCredentials)\nreadConfig path = do\n  cStr <- readConfigStr path\n  pure $ parseConfig cStr\n“What’s pure $ for?”, asked Kim?\nKanye sighed. He knew the author of this post was in a hurry to move on to doing cooler stuff and didn’t want to get into monads in this post. So he bailed too.\nFirst $. That’s just a simple way of saying consider everything after as one value. For e.g. show $ SimpleJSON.readJSON s meant show (SimpleJSON.readJSON s) instead of (show SimpleJSON.readJSON) s. Kim approved. She liked $ signs.\nKanye then braced himself for his ‘simplification’ of pure.\n“You noticed that it was cStr <- readConfigStr path and not let cStr = readConfigStr path. The <- is syntax sugar which make it look like an =. But what is really happening underneath is something very similar to callbacks. The Aff String type has to be given a function to work on the String value within it. But this function can’t just be cStr -> parseConfig cStr. The function has to return back an Aff something. pure is a constructor. In this context of Aff, when we say pure something, it’s like saying new Aff(something) or in our case, it’s like saying new Aff(parseConfig(cStr))”\nKim beamed at Kanye. He looked so hot right now. She wanted him so bad."
  },
  {
    "objectID": "posts/purescript-twitter.html#bearer-token-from-twitter.",
    "href": "posts/purescript-twitter.html#bearer-token-from-twitter.",
    "title": "Introduction to PureScript: Twitter Search API",
    "section": "Bearer Token from Twitter.",
    "text": "Bearer Token from Twitter.\nGreat, that gave her the credentials but she needed a bearer token from Twitter which she would then use to get the results. How does one call the Twitter endpoint in PureScript? She beckoned her secretary for technology to find her a library. Her secretary came back running.\n“I found a library called Milkis… again by Justin Woo!”\nKim’s eyes sharpened with intent. She wondered out aloud, “Do you think this Justin guy is a celebrity in the PureScript world? Hmmmm make my agent call his agent. Let’s do a reality show together.”\nKim first created a method to construct the authorization string from the credentials and encode it in Base64. The <> was like an append operator.\nimport Data.String.Base64 as S\nauthorizationStr :: TwitterCredentials -> String\nauthorizationStr credentials =\n  S.encode $ credentials.consumer_key <> \":\" <> credentials.consumer_secret\n\nShe then made a simple fetch helper method from Milkis.\nimport Milkis as M\nimport Milkis.Impl.Node (nodeFetch)\n\n\nfetch :: M.Fetch\nfetch = M.fetch nodeFetch\nShe then created a method to get the bearer token string or return a string as error(in the Left part of the code).\nimport Milkis as M\nimport Effect.Aff (Aff, attempt)\n\ngetTokenCredentialsStr :: String -> Aff (Either String String)\ngetTokenCredentialsStr basicAuthorizationStr = do\n    let\n      opts =\n        { body: \"grant_type=client_credentials\"\n        , method: M.postMethod\n        , headers: M.makeHeaders { \"Authorization\": basicAuthorizationStr\n                                 , \"Content-Type\": \"application/x-www-form-urlencoded;charset=UTF-8\"\n                                 }\n\n        }\n    _response <- attempt $ fetch (M.URL \"https://api.twitter.com/oauth2/token\") opts\n    case _response of\n      Left e -> do\n        pure (Left $ show e)\n      Right response -> do\n        theText <- M.text response\n        pure (Right theText)\n\nNow to bring it all together.\n\ntype BearerAuthorization =\n  { token_type :: String\n  , access_token :: String\n  }\n\nbasicHeader :: String -> String\nbasicHeader base64EncodedStr = \"Basic \" <> base64EncodedStr\n\ntoBearerAuthorization :: String -> Either String BearerAuthorization\ntoBearerAuthorization tokenString = do\n  case SimpleJSON.readJSON tokenString of\n    Left e -> do\n      Left $ show e\n    Right (result :: BearerAuthorization) -> do\n      Right result\n\ngetTokenCredentials :: TwitterCredentials -> Aff (Either String BearerAuthorization)\ngetTokenCredentials credentials = do\n  tokenCredentialsStrE <- getTokenCredentialsStr $ basicHeader $ authorizationStr credentials\n  case tokenCredentialsStrE of\n    Left error -> do\n      pure (Left error)\n    Right tokenCredentialsStr -> do\n      let tokenCredentialsE = toBearerAuthorization(tokenCredentialsStr)\n      case tokenCredentialsE of\n        Left error -> do\n          pure (Left error)\n        Right authResult -> do\n          pure (Right authResult)\nGreat, we had the bearer token. It’s finally time to search for Kim Kardashian! PureScript had this interesting signature format though. What it was saying below was that showResults took as input a BearerAuthorization and a String and returned an Aff (Either String SearchResults)\nAlso, the SearchResults and Status had lots of fields but she just wanted the basic stuff.\n\ntype Status =\n  { created_at :: String\n  , id_str :: String\n  , text :: String\n  }\n\ntype SearchResults =\n  { statuses :: Array Status\n  }\n\ntwitterURL :: String -> M.URL\ntwitterURL singleSearchTerm = M.URL $ \"https://api.twitter.com/1.1/search/tweets.json?q=\" <> singleSearchTerm\n\nshowResults :: BearerAuthorization -> String -> Aff (Either String SearchResults)\nshowResults credentials singleSearchTerm = do\n  let\n    opts =\n      { method: M.getMethod\n      , headers: M.makeHeaders { \"Authorization\": \"Bearer \" <> credentials.access_token}\n\n      }\n  _response <- attempt $ fetch (twitterURL singleSearchTerm) opts\n  case _response of\n    Left e -> do\n      pure (Left $ show e)\n    Right response -> do\n      stuff <- M.text response\n      let aJson = SimpleJSON.readJSON stuff\n      case  aJson of\n        Left e -> do\n          pure $ Left $ show e\n        Right (result :: SearchResults) -> do\n          pure (Right result)\nFinally, reaching the very end to the main command!\nimport Effect.Class.Console (errorShow, log)\nimport Effect.Aff (Aff, launchAff_)\n\nmain :: Effect Unit\nmain = launchAff_ do\n  let searchTerm = \"Kim Kardashian\"\n  config <- readConfig \"./config/twitter_credentials.json\"\n  case config of\n    Left errorStr -> errorShow errorStr\n    Right credentials -> do\n      tokenCredentialsE <- getTokenCredentials credentials\n      case tokenCredentialsE of\n        Left error ->\n          errorShow error\n        Right tokenCredentials -> do\n          resultsE <- showResults tokenCredentials searchTerm\n          case resultsE of\n            Left error ->\n              errorShow error\n            Right result ->\n              log $ show $ \"Response:\" <> (show result.statuses)\n\nlaunchAff_ was required because the entire computation returned Aff something but main was of type Effect Unit. So launchAff_ just converted Aff something to Effect Unit\nAs Kim beamed with pride at her code, she flashed her eyes at Kanye and asked him, “Isn’t the code beautiful?”\nKanye gazed into her eyes and said, “Actually, it sucks. There are so many case statements in that code that I feel cross eyed.”\nAnd the next thing Kanye knew, was that he was flat on the ground, his jaw felt like it had been displaced and he was seeing double.\nFor there are three things you don’t tell your wife:\n\nHoney, you have gained weight\nYour code sucks\nI miss my mother’s cooking.\n\nAs Kanye massaged his jaw, he muttered, “.. I guess she does not want to know about the ExceptT Monad..”"
  },
  {
    "objectID": "posts/introducing-mercylog.html",
    "href": "posts/introducing-mercylog.html",
    "title": "Introduction to Datalog(Bashlog) in Python.",
    "section": "",
    "text": "TLDR:\nDatalog is like SQL + Recursion. It’s derivatives have reduced the code base by 50% or more.\n\n\nDatalog\nToday, I would like to explore a constrained language called Datalog. It’s a constrained form of Prolog and may not be as expressive as C++ or Python. But it’s derivatives have been known to reduce the numbers of lines of code down by 50% or more(Overlog, Yedalog).\nLet’s get started:\nDatalog has a minimalist syntax which I love. Let’s take an example. Suppose our data is about fathers and sons, mothers and daughters. If we had an excel sheet, we would enter the data like:\nFather  Son\nAks     Bob\nBob     Cad\nYan     Zer\nand another excel sheet for mothers and daughters:\nMother  Daughter\nMary    Marla\nMarla   Kay\nJane    Zanu\nIn Datalog, we express the same data as(together):\nfather('Aks', 'Bob')\nfather('Bob', 'Cad')\nfather('Yan', 'Zer')\nmother('Mary', 'Marla')\nmother('Marla', 'Kay')\nmother('Jane', 'Zanu')\nHere we are trying to say Aks is the father of ‘Bob’ and ‘Bob’ is the father of ‘Cad’. The datum father(‘Aks’, ‘Bob’) is called a fact i.e. it is true.\nSo Datalog can be used to express data. Not very interesting so far but a building block. These facts above can also be viewed as the existing state of the system, like we store state in files, or databases.\nBut that’s not enough. What about code? For Datalog, code are specified as rules to be applied declaratively.\nLet’s say our program needs to find out who’s a grandfather. We could write a rule like: ‘A person X is the grandfather of Z if X is the father of Y and Y is the father of Z’. In Datalog, this rule is written as:\ngrandfather(X, Z) :- father(X,Y), father(Y, Z)\nThe LHS (i.e. grandfather) is known as the head and the RHS after the :- is known as the body\nX, Y, Z are special variables called logic variables. They are different from regular variables. They are more used to represent a pattern or to link the head and the body.\nTo further understand logical variables, consider these two rules:\ngrandfather(X, Z) :- father(X,Y), father(Y, Z)\ngrandmother(X, Z) :- mother(X,Y), mother(Y, Z)\nHere the X, Z and Y used in grandfather are completely different from the X, Y and Z in grandmother. In rules, the variables only make sense in that single rule. So we can reuse the same logic variables in different rules without worrying that they have some logical connection.\nThe next concept is queries. How do we feed input and get back some output. Queries are similar to rules but without a head.\nfather(X, 'Aks'), mother(Z, 'Aks')\n \nwe mean, find the mother and father of ‘Aks’ or\nfather(X, 'Raj'), mother(Z, 'Raj')\nwe mean, find the father and mother of ‘Raj’\nSuppose, we want to say find the mother and father of all the children in the database, we make the query\nfather(X, Y), mother(Z, Y) \nDatalog will link Y for all mother and father facts and find the mothers and fathers for a child. It will not mix up fathers and mothers :)\nNow, If you opened a datalog interpreter and fed the above and made the following queries, you would get the results shown after the # sign\nfather(X,_) # ['Aks', 'Bob', 'Yan']\nfather(_,X) # ['Bob', 'Cad', 'Zer']\nfather(X, Y) # [('Aks', 'Bob'), ('Bob', 'Cad'), ('Yan', 'Zer')\nfather(X, 'Zer'), father('Zer', Y) # [] as there are no facts that match the query\ngrandfather(X, Y) # [('Aks', 'Cad')]\ngrandfather(X,_) # ['Aks']\nHere ’_’ is a special variable indicating that you don’t care for the result.\nI was always interested in the Datalog syntax and it’s power. I kept delaying it until I met Bashlog. Because, the syntax of datalog is so simple, it makes it easy to write interpeters for different targets. What Bashlog did was take Datalog syntax and convert it to bash scripts! Because, it used awk(mawk actually), sed, grep, which are tuned for high performance on Unix like platforms, it was incredibly fast in parsing big text files, comparable with all the specialized databases out there. Just Bash Scripts. It blew my mind. So if you are interested in pure Datalog, check out Bashlog\nWith Bashlog, you can run any bash like command and read that using Bashlog. Imagine there was a file(‘~/data.tsv’) with tab separated values of\nAks Bob\nBob Cad\nYan Zer\nWe could read that data like:\nfacts(F, S) :~ cat ~/data.tsv\nfather(X, Y) :- facts(X, Y)\nAnd then we proceed the same manner like before. What’s awesome is that you can run any Unix command(e.g. ls -l) as long as it returns an output of tab separated values.\nBut I wanted to use Datalog in my day to day programming. I wanted to see if I could use and leverage Datalog along with Python. Some benefits of Datalog in Python are:\n\nModularity. How do we abstract out patterns in our rules and facts.\nPossible access to exisitng rich source of libraries.\n\nSo I built Mercylog in Python.\nSo let’s translate the above rules to Mercylog syntax.\n\n\nInstallation\nIf you are using the Bashlog variant, - then you need Java 8 already installed\ngit clone https://github.com/RAbraham/mercylog_tutorial.git\ncd mercylog_tutorial\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\npython tutorial.py\nThat should print\n['Aks']\n['Mary']\nRead below on the explanation and make tweaks if you want and run python tutorial.py again.\n\n\nUsage\nimport mercylog\nm = mercylog.BashlogV1()\n\n# father('Aks', 'Bob')\n# father('Bob', 'Cad')\n# father('Yan', 'Zer')\n# mother('Mary', 'Marla')\n# mother('Marla', 'Kay')\n# mother('Jane', 'Zanu')\n\nfather = m.relation('father')\nmother = m.relation('mother')\nfacts = [\n    father('Aks', 'Bob'),\n    father('Bob', 'Cad'),\n    father('Yan', 'Zer'),\n    mother('Mary', 'Marla'),\n    mother('Marla', 'Kay'),\n    mother('Jane', 'Zanu'),\n] \n# grandfather(X, Z) :- father(X,Y), father(Y, Z)\ngrandfather = m.relation('grandfather')\nX, Y, Z = m.variables('X', 'Y', 'Z') \ngrandfather(X, Z) <= [father(X, Y), father(Y, Z)]\nWhile in Datalog, you don’t have to explicitly state the variables and the relation, as it is baked in to the language, in our library in Python, we need to (e.g. X, Y, Z and father, grandfather)\nMaking a query in python has the following syntax\nm.run(facts, rules, query)\nA concrete example would be:\nm.run(facts, rules, grandfather(X, Y)) # which gives [('Aks', 'Cad')]\nm.run(facts, rules, father(X,_)) # ['Aks', 'Bob', 'Yan']\nm.run(facts, rules,father(_,X)) # ['Bob', 'Cad', 'Zer']\nm.run(facts, rules,father(X, Y)) # [('Aks', 'Bob'), ('Bob', 'Cad'), ('Yan', 'Zer')\nm.run(facts, rules, granfather(X,_)) # ['Aks']\nCreating this DSL in python gives us some unique benefits. For e.g if we had these two relations\npaternal_grandfather = m.relation('paternal_grandfather')\nmaternal_grandmother = m.relation('maternal_grandmother')\nfather = m.relation('father')\nmother = m.relation('mother')\n\nX, Y, Z = m.variables('X', 'Y', 'Z')\nrules = [\n    paternal_grandfather(X, Z) <= [father(X, Y), father(Y, Z)],\n    maternal_grandmother(X, Z) <= [mother(X, Y), mother(Y, Z)]\n] \nIf you notice, the rule for paternal_grandfather and maternal_grandmother are very similar. I could perhaps encapsulate that into a function. I’ll use the word transitive though I believe it is incorrect to use it.. but I don’t know what to call this for now. Rewriting the above code:\n\ndef transitive(head, clause):\n    X, Y, Z = m.variables('X', 'Y', 'Z')\n    return head(X, Z) <= [clause(X, Y), clause(Y, Z)]\n\npaternal_grandfather = m.relation('paternal_grandfather')\nmaternal_grandmother = m.relation('maternal_grandmother')\nfather = m.relation('father')\nmother = m.relation('mother')\n\n\nrules = [\n    transitive(paternal_grandfather, father),\n    transitive(maternal_grandmother, mother)\n]     \nIn this way, using Python, we have modularized a pattern using the transitive function.\nLet’s recap the benefits of Mercylog - Simple Syntax. All you need to know is facts and rules. Because of such simplicity, it is also easy to build compilers for it. - Expressive. Rules give a powerful mechanism - Declarative. Like SQL but more expressive. So we can optimize it’s engines without affecting the code\nI’ll continue to update you with my future learnings!"
  },
  {
    "objectID": "posts/2021-12-13-tabbyql-design.html",
    "href": "posts/2021-12-13-tabbyql-design.html",
    "title": "TabbyQL. A visual query language for databases",
    "section": "",
    "text": "Objective\nThis post is to present, for feedback, an alternative to SQL: TabbyQL, a visual dataflow query language.\n\n\nMotivation\n(ask me and I’ll write something up)\n\n\nDesign"
  },
  {
    "objectID": "posts/purescript-serverless.html",
    "href": "posts/purescript-serverless.html",
    "title": "PureScript on AWS Lambda. Using Express and Serverless",
    "section": "",
    "text": "This post is an ported, edited version of the original\n\nPurpose\nThis post shows you how to run PureScript 0.12 on AWS Lambda, using Express on the Serverless platform.\n\n\nSetup\nThe final output of this article is also a repo\n\n\nPrerequisites.\nYou should have nodejs (i.e. nvm, npm) setup on your machine, preferably 8.10. AWS Lambda uses 8.10. I’m new to nodejs land but I’m guessing that higher minor versions should be ok.\n\n\nSoftware Setup\nnpm install -g purescript\nnpm install -g pulp bower\nnpm install -g serverless\n\n\nAWS Credentials\nIf you haven’t already, generate an AWS key and secret. This user must have AdministratorAccess permissions. Here are the docs or a slightly outdated video. For the video, follow on to 1:40 and ignore the Serverless Dashboard instructions(around 0:58 to 1:00), we are going to do that on the command line.\nserverless config credentials --provider aws --key your-aws-key --secret your-aws-secret\n\n\nProject Setup\nmkdir hello-purescript-serverless\ncd hello-purescript-serverless\nnpm init # fill in as directed\npulp init\npulp build\n\nIf all goes well, you should see something like below:\n* Building project in /Users/rabraham/Documents/dev/purescript/hello-purescript-serverless\nCompiling Data.Symbol\nCompiling Type.Data.RowList\n...\nCompiling Main\nCompiling PSCI.Support\nCompiling Effect.Class.Console\n* Build successful.\n\nNow let’s install our project specific packages.\nbower install --save purescript-aws-lambda-express purescript-express\nnpm install --save aws-serverless-express express\nnpm install serverless-offline --save-dev\n\npurescript-express is a wrapper on express while purescript-aws-lambda-express provides the wrapper for AWS Lambda. serverless-offline allows us to test the code locally before deploying it to AWS Lambda.\nAt time of writing, purescript-express has an issue where we have to install the following two packages too. Try a pulp build right now and if that fails, run the following commands\nbower install --save purescript-test-unit\nbower install --save purescript-aff\n\nLet’s build it\npulp build\nYou should see some warnings but at the end, you should see * Build successful.\n\n\nMain Course\nIn your src/Main.purs, delete the previous code and paste the following:\nmodule Main where\n \nimport Node.Express.App (App, get)\nimport Node.Express.Handler (Handler)\nimport Node.Express.Response (sendJson)\nimport Network.AWS.Lambda.Express as Lambda\n \n-- Define an Express web app\n \nindexHandler :: Handler\nindexHandler = do\n  sendJson { status: \"ok\" }\n \napp :: App\napp = do\n  get \"/\" indexHandler\n \n-- Define the AWS Lambda handler\n \nhandler :: Lambda.HttpHandler\nhandler =\n  Lambda.makeHandler app\n\nBuild Your App\npulp build\n\n\nServerless Setup\nIn the root of your project, create the file serverless.yml and paste the following:\nservice: purescript-aws-lambda-express-test\n \nprovider:\n  name: aws\n  runtime: nodejs8.10\n  memorySize: 128\n  # stage: ${opt:stage dev}\n  region: us-east-1\n \nfunctions:\n  lambda:\n    handler: output/Main/index.handler\n    events:\n      - http:\n          path: / # this matches the base path\n          method: ANY\n      - http:\n          path: /{any+}\n          method: ANY\n \nplugins:\n  - serverless-offline\nLet’s test this locally: On one terminal.\nserverless offline start\nOpen another terminal and do:\ncurl http://localhost:3000\nYou should see {\"status\":\"ok\"}\n\n\nDeploy\nOnce it works locally, let’s deploy to AWS.\nserverless deploy -v\nOutput should look like something below. Note, your endpoint will be different:\nServerless: Packaging service\n...\nService Information\nservice: purescript-aws-lambda-express-test\nstage: dev\nregion: us-east-1\n...\nStack Outputs\n...\nServiceEndpoint: https://l4qajv7v95.execute-api.us-east-1.amazonaws.com/dev\n....\n\nCopy the link shown as ServiceEndpoint and you know what to do!\ncurl https://l4qajv7v95.execute-api.us-east-1.amazonaws.com/dev\nOutput:\n{\"status\":\"ok\"}%\n\n\nUndeploy\nserverless remove -v\nI hope this enables you to make PureScript web applications! Thanks to purescript-express and purescript-aws-lambda-express for making this possible."
  },
  {
    "objectID": "posts/2021-10-29-dsl-datalog-ohm-glue.html",
    "href": "posts/2021-10-29-dsl-datalog-ohm-glue.html",
    "title": "A toy Datalog parser using Ohm and Glue.",
    "section": "",
    "text": "This post shows you how to create a toy Datalog, a laughably incomplete subset of the Datalog language, using an Ohm parser for the syntax and a Glue parser for the semantics. The final output is a Python program."
  },
  {
    "objectID": "posts/2021-10-29-dsl-datalog-ohm-glue.html#creating-a-parse-tree",
    "href": "posts/2021-10-29-dsl-datalog-ohm-glue.html#creating-a-parse-tree",
    "title": "A toy Datalog parser using Ohm and Glue.",
    "section": "Creating a Parse Tree",
    "text": "Creating a Parse Tree\nAlright, time to dive in. We build the grammar for our toy Datalog. Ohm has an excellent grammar editor which allows one to write a grammar, test it with examples and see the ‘concrete syntax tree’ i.e. how a concrete example of Datalog code breaks out into a parse tree.\n\n\n\nOhm Editor\n\n\nLet’s start with the smallest Ohm grammar\ndatalog {\n Program = Statement+\n}\nThis is just saying that my Datalog code is a Program with one or more Statements. The + indicates one or more.\nWhat’s a Statement? Well, in the simplest case, we have a fact, e.g. parent(abe, bob). and then we have a rule e.g. father (X, Y) :- man(X), parent(X, Y).. I see a sub pattern there as relation (Variable1, Variable2). I’ll call this a Clause and then I’ll worry later how to parse it. So the rule becomes Clause :- Clause, Clause. or even Clause :- Clause, Clause, Clause... We have to capture the pattern , Clause 0 or many times. The way we do it in Ohm is like this.\nStatement =\n    Clause \".\"                          -- fact\n  | Clause \":-\" Clause CommaClause* \".\" -- rule\n\nCommaClause = \",\" Clause\n-- fact and -- rule look like comments but they are more than that. I’ll explain later below.\nCool, what’s a Clause? It’s like relation(Variable1, Variable2,..,VariableN). I’m going to worry about the ‘variable’ number of variables later. For now, I’ll call it\nClause = Relation \"(\" IDList \")\"\nOk, What’s a Relation? It’s a lower case string e.g. father and it has to start with a lower case letter(a Datalog convention). We need to capture single letter relations e.g a in a(X,Y) or bigger relations e.g. father\nRelation = LowerCaseIdent\nLowerCaseIdent = \"a\" .. \"z\" identRest*\nidentRest = \"0-9\" | \"_\" | \"A\" .. \"Z\" | \"a\" .. \"z\"\nidentRest captures the pattern that the rest of the identifier can can either be a number, underscore or upper case or lower case letters.\nNow, IDList. You’ll notice a trend here with CommaClause and IDList. It has the same feeling like designing functions. Often, I’ll write pseudocode on how I want a function to look and I’ll design the subroutines without actually implementing them. Just the subroutine calls. Once I like what I see, I’ll then go ahead and implement each subroutine. It feels similar here.\nIDList has the same feel like Clause, doesn’t it?\nIDList = ID CommaID*\nCommaID = \",\" ID\nAn ID is going to be either a literal(or constant) like abe in man(abe) or a variable like X and Y in father(X, Y). A variable starts with a capital letter as per Datalog convention.\n\nID = Variable | Literal\nVariable = CapitalizedIdent\nLiteral = LowerCaseIdent \n#LowerCaseIdent  and identRest were already explained before\nCapitalizedIdent = \"A\" .. \"Z\" identRest*\n\nSo there you go, I think that’s it. Let’s see the whole ohm grammar file.\ndatalog {\nProgram = Statement+\nStatement =\n    Clause \".\"                          -- fact\n  | Clause \":-\" Clause CommaClause* \".\" -- rule\n\nClause = Relation \"(\" IDList \")\"\nCommaClause = \",\" Clause\n\nRelation = LowerCaseIdent\nIDList = ID CommaID*\nID = Variable | Literal\nCommaID = \",\" ID\n\nVariable = CapitalizedIdent\nLiteral = LowerCaseIdent\n\nLowerCaseIdent = \"a\" .. \"z\" identRest*\nCapitalizedIdent = \"A\" .. \"Z\" identRest*\n\nidentRest = \"0-9\" | \"_\" | \"A\" .. \"Z\" | \"a\" .. \"z\"\n}"
  },
  {
    "objectID": "posts/2021-10-29-dsl-datalog-ohm-glue.html#glue",
    "href": "posts/2021-10-29-dsl-datalog-ohm-glue.html#glue",
    "title": "A toy Datalog parser using Ohm and Glue.",
    "section": "Glue",
    "text": "Glue\nNow that we have the Ohm grammar, it’s time to use it to assign some meaning to the parse tree that it’ll generate from our sample Datalog program. Ohm actually already allows us to do that too.\nHowever, Paul Tarvydas found that he was writing a lot of boilerplate code and decided to ….. write another DSL for it! It’s called Glue.\nCAUTION: Glue is a prototype and does not cover all edge cases.\nThe way Glue works is you take all the symbols in your Ohm grammar e.g. Clause, ID and write a corresponding piece of JavaScript code to be generated. Glue also simplifies it in the way that you can just write the string of code(Python in my case) that you want to generate. Let’s start with a dummy example.\nProgram [@Statement] = [[ ${Statement} ]]\nHere, Program and Statement are from the Ohm grammar above. Since we have multiple statements, we use @ to indicate that. The code to be generated is between [[ and ]]. And we refer to the code with JavaScript interpolation code ${}. What is the value of Statement above? Well, it’ll be populated ‘somehow’ by Ohm with the values from the rest of the grammar.\nI think the above example was too easy for you :) . Let’s show you what I have to actually do.\nIf you look at my required Python Mercylog code above, I need to first generate some generic, wrapper like Python code like mercylog import statements and initialization no matter what the Datalog code is to be generated.\n\nfrom mercylog import R, V, and_, db\nfb = [ .. statements ..] # refer to facts_rules_query above \nds = db() \nresult = ds(fb)\nprint(result.df()) \nThe way we specify that in Glue is:\nProgram [@Statement] = [[from mercylog import R, V, and_, db\\nfb = \\[ ${Statement}\\] \\nds = db() \\nresult = ds(fb) \\nprint(result.df()) ]]\nIt may not look very clean and Paul may have some way to improve on this but I just wanted to give you an idea.\nWe need to escape special characters in Glue. For e.g., I have to use [] for Python but that means something in Glue too so I have to escape it e.g. \\[\nWhere do we get statements from? Well, there was something I didn’t mention before, that it’s time to talk about now. Notice the -- rule and -- fact in our Ohm grammar?\nStatement =\n    Clause \".\"                          -- fact\n  | Clause \":-\" Clause CommaClause* \".\" -- rule\n\nThey look like comments but they are called case labels in Ohm. They are a way to specify different execution paths for Glue (actually, for Ohm, which Glue uses underneath). So we have two entries in Glue for Statement, Statement_fact and Statement_rule\nNote, that the number of parameters for e.g. Statement_fact match closely with how it’s grammar is specified. Using k in kperiod is just a convention, I think, to indicate that it’s a literal unlike the others.\nIf it’s a fact, I just have to add it to the list fb. The code containing fb is generated above for Program  [@statement]. So I take clause which will be generated later and just add a , to it, as I’m adding it to a Python list which wants a comma between list elements. For rules, I have to convert Statement_rule to the corresponding mercylog  statement i.e. clause1 << and_(...)\nStatement_fact [clause kperiod] = [[${clause},\\n]]\nStatement_rule [clause1 kcolondash clause @commaclause kdot] = [[${clause1} << and_(${clause}${commaclause}),\\n]]\n\nI think the rest of the code follows the same principles explained above.\nHere’s the final Glue Code.\nProgram [@Statement] = [[from mercylog import R, V, and_, db\\nfb = \\[ ${Statement}\\] \\nds = db() \\nresult = ds(fb) \\nprint(result.df()) ]]\nStatement_fact [clause kperiod] = [[${clause},\\n]]\nStatement_rule [clause1 kcolondash clause @commaclause kdot] = [[${clause1} << and_(${clause}${commaclause}),\\n]]\n\nClause [predicate klpar idlist krpar] = [[${predicate}(${idlist})]]\n\nCommaClause [kcomma clause] = [[\\,${clause}]]\n\nRelation [lowercaseident] = [[R.${lowercaseident}]]\nIDList [id @commaid] = [[${id}${commaid}]]\n\nID [id] = [[${id}]]\nCommaID [kcomma id] = [[\\, ${id}]]\n\nVariable [capident] = [[V.${capident}]]\nLiteral [lowercaseident] = [[\"${lowercaseident}\"]]\n\nLowerCaseIdent [c @cs] = [[${c}${cs}]]\n\nCapitalizedIdent [c @cs] = [[${c}${cs}]]\n\nidentRest [c] = [[${c}]]"
  },
  {
    "objectID": "posts/2021-10-29-dsl-datalog-ohm-glue.html#code-generation",
    "href": "posts/2021-10-29-dsl-datalog-ohm-glue.html#code-generation",
    "title": "A toy Datalog parser using Ohm and Glue.",
    "section": "Code Generation",
    "text": "Code Generation\nSo we have the Ohm and Glue grammars. We can now take a Datalog program and generate Python code. Using the pfr tool in the repo parse tool which reads an Ohm, Glue file and input source language file, we can generate the Python code.\npfr test.datalog datalog.ohm mercylog.glue > test.py \n\nNOTE: I had to hack the code above a bit to make it work for me but Paul may have updated his repo to make it work seamlessly for all. The emphasis here is not on having production ready tools but to rapidly explore ideas in DSL building and get feedback.\nThat generates\nfrom mercylog import R, V, and_, db\nfb = [ R.parent(\"abe\", \"bob\"),\nR.parent(\"abby\", \"bob\"),\nR.parent(\"bob\", \"carl\"),\nR.parent(\"bob\", \"connor\"),\nR.parent(\"beatrice\", \"carl\"),\nR.man(\"abe\"),\nR.man(\"bob\"),\nR.woman(\"abby\"),\nR.woman(\"beatrice\"),\nR.father(V.X, V.Y) << and_(R.man(V.X),R.parent(V.X, V.Y)),\nR.father(V.X, V.Y),\n] \nds = db() \nresult = ds(fb) \nprint(result.df()) \nNot that it’s not indented properly, nor have I done any optimizations. For e.g. V.X could be refactored to X=V.X like in the Python code showed in the very beginning. But that’s another exercise.\nNow to test my code, I have to create a project with mercylog in my dependencies and then do\npython test.py\n\nand I’ll see\n      Y    X\n0  connor  bob\n1     bob  abe\n2    carl  bob"
  },
  {
    "objectID": "posts/fp-pyrsistent.html",
    "href": "posts/fp-pyrsistent.html",
    "title": "Reading code easily with immutable values(Pyrsistent).",
    "section": "",
    "text": "TLDR:\nUse data structures that don’t change once created(using Pyrsistent) to make it easy to understand and maintain code.\n\n\nPurpose.\nWhen a data structure(e.g. dict) once created, does not change, it allows us to read code with more confidence.\nFor e.g. Let’s say you have a customer variable in your code and you are tracking it’s value by reading the code. What do you reason is the value of customer below?\ncustomer = dict(name=\"Rajiv\", age=40)\nsome_function(customer)\nprint(customer)\nIn the above code, we can’t say. In Python, for most default data structures like dict, it is possible that some_function could have changed the value of customer. So, we have to dig in and read the code of some_function to be fully sure. If the code of some_function was below:\ndef some_function(a):\n    a1 = a\n    a1['name'] = 'NewRajiv' # Changing the values. blasphemy\n    # do something with a1\nthen print(customer) would display {'name': 'NewRajiv', 'age': 40}.\nIf you are lucky, some_function does not pass it forward to other functions! Or else, you would have to dig in and read those functions too :). Now that would suck. Unless, it is the intention that the customer field should be mutated but in most cases, one does not expect it to be so(in other languages,naming conventions are used to indicate if that is the case). A knowledgeable programmer may make a copy(via the copy.deepcopy()) and work on the copy to prevent her code from affecting the client code but I have not been that knowledgeable programmer :)\nWhat if we could use a data structure that once created, cannot be changed i.e. it is immutable. Let’s check out a library called pyrsistent that gives us such data structures.\nfrom pyrsistent import m # m is like a dictionary\n\ncustomer1 = m(name='Rajiv', age=40)\ncustomer2 = customer1.set(name='NewRajiv')\nprint(customer1) # pmap({'age': 40, 'name': 'Rajiv'})\nprint(customer2) # pmap({'age': 40, 'name': 'NewRajiv'})\nWhen we specify a different value(‘NewRajiv’), a copy is created with that new value and assigned to customer2. customer1 still retains the value it was first assigned. Now, let’s go back to our previous code example and modify it a bit for pyrsistent\nfrom pyrsistent import m # m is like a dictionary\n\ndef some_function(a):\n    a1 = a.set('name', 'NewRajiv')\n    # do something with a1 \n\ncustomer = m(name=\"Rajiv\", age=40)\nsome_function(customer)\nprint(customer)\n\nprint(customer) would display {'name': 'Rajiv', 'age': 40}, the value set in our code. So, we can safely reason about our code and what it’s doing without worrying about it changing inside some_function. We don’t have to even look into some_function in this case. Trust me, when you can’t run that snippet of code to see what the actual values are, this feature makes life so easy :).\npyrsistent also has support for other common data structures(i.e. lists, sets) and much much more. Most of these pyrsistent data structures are drop in replacements for their Python counterparts when it comes to accessing the data.\nFrom the pyrsistent docs:\nfrom pyrsistent import v  # like a list\n\na = v(1, 2, 3)\nb = a.append(4)\n\nprint(b[1])  # 2\nprint(b[1:3])  # pvector([2, 3])\nprint([2 * x for x in b])  # [2, 4, 6, 8]\n\n\nOn Speed and Memory\nI simplified(ok, I lied) when I said that pyrsistent makes a copy of the data structure. Such a practice would be a waste of memory and time if we copy over every huge data structure. pyrsistent mitigates that to a great extent by not just blindly copying data structures and then making the modifications. It tries to be intelligent by sharing the common parts between a original data structure and the new modified copy to save on memory and time.\nLet’s take an example(Credit: Wikipedia: Persistent Data Structures). Ah, you now are exposed to what this is really called. This concept is called persistent data structures or functional data structures.\nNOTE: The below example is just to explain the concepts and such a binary search tree is not part of pyrsistent.\nLet’s say you had a binary search tree(xs) which was a persistent data structure:\n\n\n\n\n\nalt text\n\n\nNow if you added a node e to that data structure, e.g. ys = insertNode(xs, e) A naive implementation would copy the data structure and then insert e at the appropriate location. In a persistent data structure approach, it would be:\n\n\n\n\n\nalt text\n\n\nSince e falls into the right side of the tree(i.e. the tree with g as the root), the tree with root b is not affected and hence can be reused. You can see a arrow from d' to b indicating that.\nThis reuse saves memory space, and saves time not done copying as it merely uses pointers to refer to the unchanged data.\nNote: just because a data structure is being reused does not mean modifications to one can affect another. They are immutable and hence cannot be modified. If you do modify, a new structure is created like above.\nNote on Note: pyrsistent tries to be as fast as possible and has comparable speeds to the norm for most cases. The complexity of most operations are well described in their docs. ### Nested Transformations\nWhat if we have to update a nested value in a data structure while maintaining immutability. pyrsistent has a method transform for that. How I would normally do it\nimport copy\nm4 = dict(a=1, b=6, c=[1, 2])\n# I want to update c[1] to 17\nm4_new = copy.deepcopy(m4) \nm4_new['c'][1] = 17\nFrom their docs,\nfrom pyrsistent import m  # m is like a dictionary\nfrom pyrsistent import v # m is like a list\nm4 = m(a=5, b=6, c=v(1, 2))\nm4_new = m4.transform(('c', 1), 17)\nprint(m4_new) # pmap({'a': 5, 'c': pvector([1, 17]), 'b': 6})\n\n\nUpdating dictionaries\nOne thing I do very often is merging dictionaries. For e.g., I may have to construct my configuration taking the the following sources with the earliest being the highest priority. * Environment variables * File configuration * Default configuration\nHow I would normally do it.\ndefault_conf = dict(database_url='dev_url', user='postgres', port=5432)\n# Imagine file_conf below was extracted from a file\nfile_conf = dict(user='test_user', port=5433)\n# Imagine env_conf below was constructed from environment variables\nenvironment_conf = dict(database_url='test_url')\nfinal_conf = {**default_conf, **file_conf, **environment_conf}\n\nprint(final_conf) # {'database_url': 'test_url', 'user': 'test_user', 'port': 5433}\nThat’s great for 99% of the cases I would think :). But for the sake of discussion, perhaps if you had HUGE dictionaries(e.g. merging all the data you scrapped illegally from some website ;) ), that would be some duplication of data in memory. In pyrsistent:\nfrom pyrsistent import m\ndefault_conf = m(database_url='dev_url', user='postgres', port=5432)\n# Imagine file_conf below was extracted from a file\nfile_conf = m(user='test_user', port=5433)\n# Imagine env_conf below was constructed from environment variables\nenvironment_conf = m(database_url='test_url')\nfinal_conf = default_conf + file_conf + environment_conf\n\nprint(final_conf) # pmap({'database_url': 'test_url', 'user': 'test_user', 'port': 5433})\nI hope this is enough to get you started in a better coding experience :). There are many other wonderful features in pyrsistent like having the above behaviour for records(PRrecord) and clases(PClass) and many more advanced features. I’ll leave that for another post.\nSo head out to pyrsistent and check it out. And if you like it, don’t forget to star! It’s a wonderful piece of engineering whose authors that we should applaud and support."
  }
]